# Code Review Report - Mini ChatGPT Assignment

## ‚úÖ Requirements Met

### Docker Setup

- ‚úÖ All Dockerfiles present (mock-llm, backend, frontend)
- ‚úÖ docker-compose.yml configured with all services
- ‚úÖ Database container included (PostgreSQL)
- ‚úÖ Health checks implemented

### LLM Adapter System

- ‚úÖ Pluggable adapter pattern implemented
- ‚úÖ Environment variable switching (`LLM_PROVIDER`)
- ‚úÖ Both mock and Ollama adapters exist
- ‚úÖ Same response shape (`{ completion: string }`)

### Core Features

- ‚úÖ Basic chat UI with send/cancel
- ‚úÖ Input disabled during in-flight sends
- ‚úÖ Conversation list with create/delete
- ‚úÖ Conversation history persistence
- ‚úÖ Optimistic delete with 5-second undo
- ‚úÖ Cursor-based pagination for messages
- ‚úÖ Cancel aborts backend‚ÜíLLM requests
- ‚úÖ Error handling and retries

### Storage

- ‚úÖ Real database (PostgreSQL)
- ‚úÖ Prisma migrations
- ‚úÖ Data persists across restarts

### API Endpoints

- ‚úÖ POST /api/conversations (201 response)
- ‚úÖ GET /api/conversations (200 response)
- ‚úÖ DELETE /api/conversations/:id (204 response)
- ‚úÖ POST /api/conversations/:id/messages (200 response)
- ‚úÖ GET /api/conversations/:id/messages with pagination

### Health & Readiness

- ‚úÖ /healthz endpoint
- ‚úÖ /readyz endpoint (checks DB)

### UI/UX

- ‚úÖ Mobile responsive (MUI breakpoints, drawer on mobile)
- ‚úÖ Empty states
- ‚úÖ Loading states
- ‚úÖ Error notifications
- ‚úÖ Keyboard navigation support

---

## ‚ùå Issues Found

### üî¥ CRITICAL ISSUES

#### 1. Mock LLM Missing Required Behavior

**Location**: `mock-llm/server.js`

**Issue**: The mock LLM server is missing the required behavior from the specification:

- ‚ùå No 10% chance of hanging forever
- ‚ùå No 20% chance of returning 500 error

**Current code** (lines 14-23):

```javascript
app.post("/complete", async (req, res) => {
  const content = (req.body && req.body.content) || "";
  console.log("Mock LLM got:", content);
  const reply = "This is a mock response from a pretend LLM.";
  const delayMs = 500 + randomInt(1500);
  await new Promise((r) => setTimeout(r, delayMs));
  return res.json({ completion: reply });
});
```

**Required code** (from spec):

```javascript
app.post("/complete", async (req, res) => {
  if (Math.random() < 0.1) return; // hang forever
  if (Math.random() < 0.2)
    return res.status(500).json({ error: "mock-llm error" });
  // ... rest
});
```

**Impact**: Cannot properly test retry logic and timeout handling.

---

#### 2. Conversation Counter Not Persistent

**Location**: `backend/src/routes/conversations.ts:8`

**Issue**: Conversation counter is stored in memory, causing:

- Counter resets on server restart ‚Üí duplicate titles
- Not persistent across service restarts
- Violates requirement for sequential titles: "Conversation #1", "Conversation #2", etc.

**Current implementation**:

```typescript
let convoCounter = 1; // ‚ùå In-memory, resets on restart

router.post("/", async (req, res) => {
  const title = `Conversation #${convoCounter++}`; // ‚ùå Not persistent
  // ...
});
```

**Required fix**: Calculate counter from database:

```typescript
router.post("/", async (req, res) => {
  const count = await prisma.conversation.count();
  const title = `Conversation #${count + 1}`;
  // ...
});
```

**Impact**: High - violates requirement for sequential, persistent titles.

---

#### 3. API Contract Mismatch

**Specification requires**:

```
GET /api/conversations/:id?messagesCursor=<cursor>&limit=<int>
```

**Implementation uses**:

```
GET /api/conversations/:id/messages?messagesCursor=<cursor>&limit=<int>
```

**Location**:

- Backend: `backend/src/routes/messages.ts:17`
- Frontend: `front/src/store/api.ts:163`

**Note**: This works correctly but doesn't match the specification. The frontend calls match the implementation, so functionally it works, but it's a spec deviation.

---

#### 4. Missing `retryAfterMs` in Error Responses

**Location**: `backend/src/routes/messages.ts` (error responses)

**Specification requires**:

```json
{
  "error": "Upstream error/timeout",
  "retryAfterMs": 1000
}
```

**Current implementation**: Error responses don't include `retryAfterMs` field.

**Example** (line 457-461):

```typescript
res.status(504).json({
  error: "Request timeout",
  message: "The AI service is taking too long to respond. Please try again.",
  // ‚ùå Missing: retryAfterMs: 1000
});
```

---

#### 5. Timeout Configuration Inconsistency

**Location**: Multiple files

**Issue**:

- Frontend timeout: 12s ‚úÖ (matches requirement)
- Mock adapter timeout: 12s ‚úÖ (matches requirement)
- Ollama adapter timeout: 120s ‚ùå (should be 12s per requirement)

**Files**:

- `front/src/store/api.ts:132` ‚Üí `timeout: 12000` ‚úÖ
- `backend/src/adapters/mockAdapter.ts:16` ‚Üí `timeout: 12000` ‚úÖ
- `backend/src/adapters/ollamaAdapter.ts:28` ‚Üí `timeout: 120000` ‚ùå

**Note**: Ollama may need longer for model loading, but per spec, client timeout should be ‚â§12s.

---

### ‚ö†Ô∏è MINOR ISSUES

#### 6. DECISIONS.md Too Brief

**Location**: `DECISIONS.md`

**Issue**: The file exists but is very minimal. Specification requires explanation of:

- ‚úÖ DB choice (briefly mentioned)
- ‚úÖ Schema & migration approach (not explained)
- ‚úÖ Retry, timeout, cancel behavior (briefly mentioned)
- ‚úÖ Pagination model (briefly mentioned)
- ‚úÖ LLM adapter structure (briefly mentioned)
- ‚ùå Tradeoffs (missing)

**Current content**: 27 lines, mostly bullet points.
**Expected**: More detailed explanations, examples, and tradeoff discussions.

---

#### 7. Mock LLM Health Endpoint Mismatch

**Issue**: docker-compose.yml expects `/health` endpoint (line 35), but the provided mock-llm/server.js doesn't include it.

**Current**: Health endpoint exists in implementation ‚úÖ
**Note**: Actually, the implementation DOES have it (line 10-12), so this is fine.

---

#### 8. Cancel Endpoint Not Implemented

**Specification mentions** (optional):

```
POST /api/conversations/:id/cancel
```

**Implementation**: Uses client-side abort (preferred method) ‚úÖ

**Note**: This is fine - spec says "Preferred: client aborts fetch (no server endpoint required)."

---

#### 9. Undo Duration Mismatch

**Specification**: "offer an Undo for ~5 seconds"

**Implementation**: `front/src/components/ConversationList.tsx:104` uses 5000ms (5 seconds) ‚úÖ

**Actually correct** - no issue here.

---

## üìä Summary Statistics

| Category                 | Status                                 |
| ------------------------ | -------------------------------------- |
| **Critical Issues**      | 5                                      |
| **Minor Issues**         | 2 (DECISIONS.md, minor clarifications) |
| **Requirements Met**     | ~85%                                   |
| **Ready for Submission** | ‚ùå No (needs fixes)                    |

---

## üîß Required Fixes Before Submission

1. **Fix Mock LLM** - Add 10% hang and 20% 500 error logic
2. **Fix Conversation Counter** - Make it persistent (query DB)
3. **Fix API Contract** - Match spec or update documentation
4. **Add retryAfterMs** - Include in all error responses
5. **Fix Ollama Timeout** - Should be 12s (or document why 120s)
6. **Expand DECISIONS.md** - Add more detail and tradeoffs

---

## ‚úÖ What's Working Well

- Excellent adapter pattern implementation
- Good error handling and logging
- Proper cursor pagination
- Optimistic UI with undo working correctly
- Mobile responsiveness implemented
- Clean code structure
- Proper TypeScript usage
- Good separation of concerns

---

## üìù Notes

The implementation is quite solid overall. The main issues are:

1. Mock LLM not matching specification behavior
2. Conversation counter not persistent
3. API endpoint path mismatch
4. Missing retryAfterMs in error responses

Most functionality works correctly, but these issues need to be addressed to fully meet the specification requirements.
