# CODE DUMP - 2025-11-01 13:10:06
# Project: mini ChatGpt
# Generated by copy_code_to_txt.py

================================================================================

--- FILE: all_code.txt ---
# Path: C:\vs code projects\mini ChatGpt\all_code.txt
# Size: 0 bytes



--------------------------------------------------------------------------------

--- FILE: all_code_new.txt ---
# Path: C:\vs code projects\mini ChatGpt\all_code_new.txt
# Size: 63624 bytes

# CODE DUMP - 2025-10-31 17:13:52
# Project: mini ChatGpt
# Generated by copy_code_to_txt.py

================================================================================

--- FILE: all_code.txt ---
# Path: C:\vs code projects\mini ChatGpt\all_code.txt
# Size: 0 bytes



--------------------------------------------------------------------------------

--- FILE: DECISIONS.md ---
# Path: C:\vs code projects\mini ChatGpt\DECISIONS.md
# Size: 607 bytes

# DECISIONS.md

## DB: PostgreSQL + Prisma
- ACID, relations, cursor pagination
- `prisma migrate deploy` in Docker
- Type-safe queries

## Backend: Node.js + Express + TypeScript
- Shared types with frontend
- Lightweight Docker image
- Full TS stack

## LLM Adapter
- `LLM_PROVIDER=mock|ollama`
- No code changes to switch
- Retry (2x), timeout (12s), cancel

## Pagination
- Cursor-based on `(createdAt DESC, id DESC)`
- Stable, no duplicates

## Resilience
- 500 → retry with backoff
- Hang → 12s timeout
- Cancel → aborts fetch

## Health Checks
- `/healthz`, `/readyz`

--------------------------------------------------------------------------------

--- FILE: docker-compose.yml ---
# Path: C:\vs code projects\mini ChatGpt\docker-compose.yml
# Size: 1735 bytes

services:
  # === DATABASE: PostgreSQL (Persistent) ===
  db:
    image: postgres:16
    container_name: chatgpt-db
    environment:
      POSTGRES_DB: chatgpt
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s

  # === MOCK LLM (Provided) ===
  mock-llm:
    build: ./mock-llm
    container_name: mock-llm
    ports:
      - "8080:8080"
    # Remove healthcheck — curl not in node:20-alpine
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8080/complete"]

  # === BACKEND: Node.js + Express + Prisma ===
  backend:
    build: ./backend
    container_name: chatgpt-backend
    ports:
      - "3001:3001"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/chatgpt?schema=public
      - LLM_PROVIDER=mock
      - MOCK_LLM_BASE_URL=http://mock-llm:8080
    depends_on:
      db:
        condition: service_healthy
      # Remove mock-llm dependency
      # mock-llm:
      #   condition: service_healthy
    command: >
      sh -c "
        echo 'Waiting for DB...' &&
        until nc -z db 5432; do sleep 1; done &&
        echo 'DB ready. Running migrations...' &&
        npx prisma migrate deploy &&
        echo 'Starting backend...' &&
        npm start
      "

  # === FRONTEND: React + MUI + Nginx ===
  frontend:
    build: ./front
    container_name: chatgpt-frontend
    ports:
      - "3000:80"
    depends_on:
      - backend

volumes:
  postgres_data:


--------------------------------------------------------------------------------

--- FILE: backend\package.json ---
# Path: C:\vs code projects\mini ChatGpt\backend\package.json
# Size: 671 bytes

{
  "name": "backend",
  "version": "1.0.0",
  "main": "dist/server.js",
  "scripts": {
    "build": "tsc",
    "start": "node dist/server.js",
    "dev": "ts-node src/server.ts",
    "migrate": "prisma migrate deploy"
  },
  "dependencies": {
    "@prisma/client": "^5.19.0",
    "axios": "^1.7.7",
    "cors": "^2.8.5",
    "express": "^4.19.2",
    "express-async-errors": "^3.1.1",
    "uuid": "^10.0.0"
  },
  "devDependencies": {
    "@types/cors": "^2.8.17",
    "@types/express": "^4.17.21",
    "@types/node": "^20.16.5",
    "@types/uuid": "^10.0.0",
    "prisma": "^5.19.0",
    "ts-node": "^10.9.2",
    "typescript": "^5.6.2"
  }
}

--------------------------------------------------------------------------------

--- FILE: backend\tsconfig.json ---
# Path: C:\vs code projects\mini ChatGpt\backend\tsconfig.json
# Size: 342 bytes

{
  "compilerOptions": {
    "target": "es2020",
    "module": "commonjs",
    "outDir": "dist",
    "rootDir": "src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true
  },
  "include": ["src"],
  "exclude": ["node_modules"]
}

--------------------------------------------------------------------------------

--- FILE: front\package.json ---
# Path: C:\vs code projects\mini ChatGpt\front\package.json
# Size: 1203 bytes

{
  "name": "front",
  "version": "0.1.0",
  "private": true,
  "dependencies": {
    "@emotion/react": "^11.14.0",
    "@emotion/styled": "^11.14.1",
    "@mui/icons-material": "^7.3.4",
    "@mui/material": "^7.3.4",
    "@reduxjs/toolkit": "^2.9.2",
    "@testing-library/dom": "^10.4.1",
    "@testing-library/jest-dom": "^6.9.1",
    "@testing-library/react": "^16.3.0",
    "@testing-library/user-event": "^13.5.0",
    "@types/jest": "^27.5.2",
    "@types/node": "^16.18.126",
    "@types/react": "^19.2.2",
    "@types/react-dom": "^19.2.2",
    "react": "^19.2.0",
    "react-dom": "^19.2.0",
    "react-redux": "^9.2.0",
    "react-scripts": "5.0.1",
    "typescript": "^4.9.5",
    "web-vitals": "^2.1.4"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test",
    "eject": "react-scripts eject"
  },
  "eslintConfig": {
    "extends": [
      "react-app",
      "react-app/jest"
    ]
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  }
}


--------------------------------------------------------------------------------

--- FILE: front\README.md ---
# Path: C:\vs code projects\mini ChatGpt\front\README.md
# Size: 2117 bytes

# Getting Started with Create React App

This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).

## Available Scripts

In the project directory, you can run:

### `npm start`

Runs the app in the development mode.\
Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

The page will reload if you make edits.\
You will also see any lint errors in the console.

### `npm test`

Launches the test runner in the interactive watch mode.\
See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.

### `npm run build`

Builds the app for production to the `build` folder.\
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.\
Your app is ready to be deployed!

See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.

### `npm run eject`

**Note: this is a one-way operation. Once you `eject`, you can’t go back!**

If you aren’t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.

Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.

You don’t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.

## Learn More

You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).

To learn React, check out the [React documentation](https://reactjs.org/).


--------------------------------------------------------------------------------

--- FILE: front\tsconfig.json ---
# Path: C:\vs code projects\mini ChatGpt\front\tsconfig.json
# Size: 682 bytes

{
  "compilerOptions": {
    "target": "es5",
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "noFallthroughCasesInSwitch": true,
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "baseUrl": "src",
    "paths": {
      "@/*": ["*"]
    }
  },
  "include": [
    "src"
  ],
  "exclude": [
    "node_modules",
    "build"
  ]
}

--------------------------------------------------------------------------------

--- FILE: mock-llm\package.json ---
# Path: C:\vs code projects\mini ChatGpt\mock-llm\package.json
# Size: 201 bytes

{
  "name": "mock-llm",
  "version": "1.0.0",
  "main": "server.js",
  "license": "MIT",
  "type": "commonjs",
  "dependencies": {
    "body-parser": "^1.20.2",
    "express": "^4.19.2"
  }
}

--------------------------------------------------------------------------------

--- FILE: mock-llm\server.js ---
# Path: C:\vs code projects\mini ChatGpt\mock-llm\server.js
# Size: 811 bytes

const express = require("express");
const bodyParser = require("body-parser");
const app = express();
app.use(bodyParser.json());

function randomInt(n) {
  return Math.floor(Math.random() * n);
}

app.post("/complete", async (req, res) => {
  if (Math.random() < 0.10) return; // hang forever
  if (Math.random() < 0.20) return res.status(500).json({ error: "mock-llm error" });

  const content = (req.body && req.body.content) || "";
  console.log("Mock LLM got:", content);

  const reply = "This is a mock response from a pretend LLM.";
  const delayMs = 500 + randomInt(1500);
  await new Promise(r => setTimeout(r, delayMs));

  return res.json({ completion: reply });
});

const port = process.env.PORT || 8080;
app.listen(port, () => console.log("mock-llm listening on", port));

--------------------------------------------------------------------------------

--- FILE: front\public\index.html ---
# Path: C:\vs code projects\mini ChatGpt\front\public\index.html
# Size: 1721 bytes

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Web site created using create-react-app"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>React App</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>


--------------------------------------------------------------------------------

--- FILE: front\public\manifest.json ---
# Path: C:\vs code projects\mini ChatGpt\front\public\manifest.json
# Size: 492 bytes

{
  "short_name": "React App",
  "name": "Create React App Sample",
  "icons": [
    {
      "src": "favicon.ico",
      "sizes": "64x64 32x32 24x24 16x16",
      "type": "image/x-icon"
    },
    {
      "src": "logo192.png",
      "type": "image/png",
      "sizes": "192x192"
    },
    {
      "src": "logo512.png",
      "type": "image/png",
      "sizes": "512x512"
    }
  ],
  "start_url": ".",
  "display": "standalone",
  "theme_color": "#000000",
  "background_color": "#ffffff"
}


--------------------------------------------------------------------------------

--- FILE: front\public\robots.txt ---
# Path: C:\vs code projects\mini ChatGpt\front\public\robots.txt
# Size: 67 bytes

# https://www.robotstxt.org/robotstxt.html
User-agent: *
Disallow:


--------------------------------------------------------------------------------

--- FILE: front\src\App.css ---
# Path: C:\vs code projects\mini ChatGpt\front\src\App.css
# Size: 564 bytes

.App {
  text-align: center;
}

.App-logo {
  height: 40vmin;
  pointer-events: none;
}

@media (prefers-reduced-motion: no-preference) {
  .App-logo {
    animation: App-logo-spin infinite 20s linear;
  }
}

.App-header {
  background-color: #282c34;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  font-size: calc(10px + 2vmin);
  color: white;
}

.App-link {
  color: #61dafb;
}

@keyframes App-logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}


--------------------------------------------------------------------------------

--- FILE: front\src\App.test.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\App.test.tsx
# Size: 273 bytes

import React from 'react';
import { render, screen } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  render(<App />);
  const linkElement = screen.getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});


--------------------------------------------------------------------------------

--- FILE: front\src\App.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\App.tsx
# Size: 939 bytes

import { Container, CssBaseline, AppBar, Toolbar, Typography, Box } from '@mui/material';
import ConversationList from './components/ConversationList';
import ChatWindow from './components/ChatWindow';
import { ThemeProvider } from '@mui/material/styles';
import theme from './theme';
import { useState } from 'react';

function App() {
  const [selectedId, setSelectedId] = useState<string | null>(null);

  return (
    <ThemeProvider theme={theme}>
      <CssBaseline />
      <AppBar position="static">
        <Toolbar>
          <Typography variant="h6">Mini ChatGPT</Typography>
        </Toolbar>
      </AppBar>

      <Container maxWidth={false} disableGutters sx={{ display: 'flex', height: 'calc(100vh - 64px)' }}>
        <ConversationList onSelect={setSelectedId} />
        <Box flex={1}>
          <ChatWindow conversationId={selectedId} />
        </Box>
      </Container>
    </ThemeProvider>
  );
}

export default App;

--------------------------------------------------------------------------------

--- FILE: front\src\index.css ---
# Path: C:\vs code projects\mini ChatGpt\front\src\index.css
# Size: 366 bytes

body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}


--------------------------------------------------------------------------------

--- FILE: front\src\index.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\index.tsx
# Size: 370 bytes

import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';
import { Provider } from 'react-redux';
import { store } from './store';

const root = ReactDOM.createRoot(
  document.getElementById('root') as HTMLElement
);
root.render(
  <React.StrictMode>
    <Provider store={store}>
      <App />
    </Provider>
  </React.StrictMode>
);

--------------------------------------------------------------------------------

--- FILE: front\src\react-app-env.d.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\react-app-env.d.ts
# Size: 41 bytes

/// <reference types="react-scripts" />


--------------------------------------------------------------------------------

--- FILE: front\src\reportWebVitals.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\reportWebVitals.ts
# Size: 425 bytes

import { ReportHandler } from 'web-vitals';

const reportWebVitals = (onPerfEntry?: ReportHandler) => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;


--------------------------------------------------------------------------------

--- FILE: front\src\setupTests.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\setupTests.ts
# Size: 241 bytes

// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';


--------------------------------------------------------------------------------

--- FILE: front\src\theme.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\theme.ts
# Size: 203 bytes

import { createTheme } from '@mui/material/styles';

export default createTheme({
  palette: {
    mode: 'light',
    primary: {
      main: '#1976d2',  // ← Fixed: 'd' not 'i'
    },
  },
});

--------------------------------------------------------------------------------

--- FILE: front\src\components\ChatWindow.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\components\ChatWindow.tsx
# Size: 3165 bytes

import { Box, CircularProgress, Button, Stack, Typography } from '@mui/material';
import { useGetConversationQuery, api } from '../store/api';
import { useAppDispatch } from '../store/hooks';
import MessageBubble from './MessageBubble';
import MessageInput from './MessageInput';
import { useEffect, useRef, useState } from 'react';
import type { Message } from '../types';

interface Props {
  conversationId: string | null;
}

export default function ChatWindow({ conversationId }: Props) {
  const dispatch = useAppDispatch();
  const [olderMessages, setOlderMessages] = useState<Message[]>([]);
  const [isLoadingOlder, setIsLoadingOlder] = useState(false);

  const {
    data: convo,
    isLoading,
    isFetching,
    error,
  } = useGetConversationQuery(
    { id: conversationId!, limit: 20 },
    { skip: !conversationId, refetchOnMountOrArgChange: true }
  );

  const scrollRef = useRef<HTMLDivElement>(null);

  // Clear older messages when switching conversation
  useEffect(() => {
    setOlderMessages([]);
  }, [conversationId]);

  useEffect(() => {
    scrollRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [convo?.messages, olderMessages]);

  const handleLoadMore = async () => {
    if (!convo?.pageInfo.prevCursor || !conversationId || isLoadingOlder) return;

    setIsLoadingOlder(true);
    try {
      const result = await dispatch(
        api.endpoints.getConversation.initiate({
          id: conversationId,
          cursor: convo.pageInfo.prevCursor,
          limit: 20,
        })
      ).unwrap();

      setOlderMessages((prev) => [...result.messages, ...prev]);
    } catch (err) {
      console.error('Failed to load older messages', err);
    } finally {
      setIsLoadingOlder(false);
    }
  };

  if (!conversationId) {
    return (
      <Box flex={1} display="flex" alignItems="center" justifyContent="center">
        <Typography color="text.secondary">Select or create a conversation</Typography>
      </Box>
    );
  }

  if (isLoading) {
    return (
      <Box flex={1} display="flex" alignItems="center" justifyContent="center">
        <CircularProgress />
      </Box>
    );
  }

  if (error) {
    return (
      <Box flex={1} p={3}>
        <Typography color="error">
          Failed to load chat: {(error as any).data || 'Try again'}
        </Typography>
      </Box>
    );
  }

  const allMessages = [...olderMessages, ...(convo?.messages || [])];

  return (
    <Box display="flex" flexDirection="column" height="100%">
      {/* Load Older Button */}
      {convo?.pageInfo.prevCursor && (
        <Stack alignItems="center" py={1}>
          <Button
            size="small"
            onClick={handleLoadMore}
            disabled={isLoadingOlder || isFetching}
          >
            {isLoadingOlder ? 'Loading...' : 'Load older messages'}
          </Button>
        </Stack>
      )}

      {/* Messages */}
      <Box flex={1} overflow="auto" p={2}>
        {allMessages.map((m) => (
          <MessageBubble key={m.id} msg={m} />
        ))}
        <div ref={scrollRef} />
      </Box>

      {/* Input */}
      <MessageInput conversationId={conversationId} />
    </Box>
  );
}

--------------------------------------------------------------------------------

--- FILE: front\src\components\ConversationList.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\components\ConversationList.tsx
# Size: 4957 bytes

import {
  List,
  ListItem,
  ListItemText,
  IconButton,
  Drawer,
  Box,
  Typography,
  Button,
  useMediaQuery,
  useTheme,
  Snackbar,
  Alert,
  ListItemButton,
} from '@mui/material';
import DeleteIcon from '@mui/icons-material/Delete';
import AddIcon from '@mui/icons-material/Add';
import MenuIcon from '@mui/icons-material/Menu';
import {
  useGetConversationsQuery,
  useCreateConversationMutation,
  useDeleteConversationMutation,
  api,
} from '../store/api';
import { useState, useEffect } from 'react';
import { useAppDispatch } from '../store/hooks';

interface Props {
  onSelect: (id: string) => void;
}

export default function ConversationList({ onSelect }: Props) {
  const theme = useTheme();
  const isMobile = useMediaQuery(theme.breakpoints.down('md'));
  const [mobileOpen, setMobileOpen] = useState(false);
  const [undoId, setUndoId] = useState<string | null>(null);
  const [undoTimer, setUndoTimer] = useState<NodeJS.Timeout | null>(null);

  const dispatch = useAppDispatch();
  const { data, isLoading, error, refetch } = useGetConversationsQuery();
  const conversations = data ?? [];
  const [create] = useCreateConversationMutation();
  const [deleteConvo] = useDeleteConversationMutation();

  const handleDelete = (id: string) => {
    // Optimistic update
    dispatch(
      api.util.updateQueryData('getConversations', undefined, (draft) => {
        if (!draft) return [];
        return draft.filter((c) => c.id !== id);
      })
    );

    const timer = setTimeout(() => {
      deleteConvo(id);
      setUndoId(null);
    }, 5000);

    setUndoId(id);
    setUndoTimer(timer);
  };

  const handleUndo = () => {
    if (undoTimer) clearTimeout(undoTimer);
    setUndoId(null);
    refetch();
  };

  useEffect(() => {
    return () => {
      if (undoTimer) clearTimeout(undoTimer);
    };
  }, [undoTimer]);

  return (
    <>
      {/* Desktop Sidebar */}
      {!isMobile && (
        <Box sx={{ width: 300, borderRight: 1, borderColor: 'divider', overflow: 'auto' }}>
          <InnerList />
        </Box>
      )}

      {/* Mobile Drawer */}
      {isMobile && (
        <>
          <IconButton onClick={() => setMobileOpen(true)} sx={{ ml: 1, mt: 1 }}>
            <MenuIcon />
          </IconButton>
          <Drawer open={mobileOpen} onClose={() => setMobileOpen(false)}>
            <Box sx={{ width: 280 }}>
              <InnerList onClose={() => setMobileOpen(false)} />
            </Box>
          </Drawer>
        </>
      )}

      {/* Undo Snackbar */}
      <Snackbar
        open={!!undoId}
        anchorOrigin={{ vertical: 'bottom', horizontal: 'center' }}
        autoHideDuration={null}
      >
        <Alert
          severity="info"
          action={
            <Button color="inherit" size="small" onClick={handleUndo}>
              UNDO
            </Button>
          }
        >
          Conversation deleted
        </Alert>
      </Snackbar>

      {/* Error State */}
      {error && (
        <Box p={2}>
          <Typography color="error">
            Failed to load conversations: {(error as any).data || 'Try again'}
          </Typography>
          <Button onClick={refetch}>Retry</Button>
        </Box>
      )}
    </>
  );

  function InnerList({ onClose }: { onClose?: () => void }) {
    return (
      <>
        <Box p={2} display="flex" justifyContent="space-between">
          <Typography variant="h6">Chats</Typography>
          <Button
            startIcon={<AddIcon />}
            onClick={async () => {
              try {
                const res = await create().unwrap();
                onSelect(res.id);
                onClose?.();
              } catch (err) {
                console.error('Create failed', err);
              }
            }}
          >
            New
          </Button>
        </Box>

        {isLoading ? (
          <Typography p={2}>Loading...</Typography>
        ) : (
          <List>
            {conversations.map((c) => (
              <ListItem key={c.id} disablePadding>
                <ListItemButton
                  onClick={() => {
                    onSelect(c.id);
                    onClose?.();
                  }}
                  sx={{ pl: 2, pr: 1 }}
                >
                  <ListItemText
                    primary={c.title}
                    secondary={
                      c.lastMessageAt
                        ? new Date(c.lastMessageAt).toLocaleString()
                        : 'No messages'
                    }
                  />
                  <IconButton
                    edge="end"
                    onClick={(e) => {
                      e.stopPropagation();
                      handleDelete(c.id);
                    }}
                  >
                    <DeleteIcon />
                  </IconButton>
                </ListItemButton>
              </ListItem>
            ))}
          </List>
        )}
      </>
    );
  }
}

--------------------------------------------------------------------------------

--- FILE: front\src\components\MessageBubble.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\components\MessageBubble.tsx
# Size: 691 bytes

import { Paper, Typography, Box } from '@mui/material';
import { Message } from '../types';

export default function MessageBubble({ msg }: { msg: Message }) {
  const isUser = msg.role === 'user';
  return (
    <Box sx={{ display: 'flex', justifyContent: isUser ? 'flex-end' : 'flex-start', mb: 1.5 }}>
      <Paper
        sx={{
          maxWidth: '75%',
          p: 1.5,
          bgcolor: isUser ? 'primary.main' : 'grey.100',
          color: isUser ? 'white' : 'text.primary',
          borderRadius: 2,
        }}
      >
        <Typography variant="body1" whiteSpace="pre-wrap">
          {msg.content}
        </Typography>
      </Paper>
    </Box>
  );
}

--------------------------------------------------------------------------------

--- FILE: front\src\components\MessageInput.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\components\MessageInput.tsx
# Size: 4910 bytes

import { TextField, IconButton, Box, CircularProgress } from "@mui/material";
import SendIcon from "@mui/icons-material/Send";
import CancelIcon from "@mui/icons-material/Cancel";
import { useSendMessageMutation } from "../store/api";
import { useRef, useState } from "react";

interface Props {
  conversationId: string;
}

export default function MessageInput({ conversationId }: Props) {
  const [send, { isLoading, reset }] = useSendMessageMutation();
  const [value, setValue] = useState("");
  const abortControllerRef = useRef<AbortController | null>(null);
  const isPendingRef = useRef(false);

  const handleSend = async () => {
    if (!value.trim() || isLoading || isPendingRef.current) return;

    console.log("[FRONTEND] Step 1: handleSend() called - starting request");

    // Mark as pending to prevent duplicate sends
    isPendingRef.current = true;

    // Create a new AbortController for this request
    const abortController = new AbortController();
    abortControllerRef.current = abortController;
    console.log(
      "[FRONTEND] Step 2: AbortController created, signal.aborted =",
      abortController.signal.aborted
    );

    try {
      // Store content before sending
      const contentToSend = value.trim();

      // Check if already aborted before sending
      if (abortController.signal.aborted) {
        console.log(
          "[FRONTEND] Step 3: Already aborted before send - exiting early"
        );
        isPendingRef.current = false;
        abortControllerRef.current = null;
        return;
      }

      console.log("[FRONTEND] Step 4: Calling RTK Query send() with signal");
      // Pass the abort signal to the mutation
      // The custom baseQuery will check if signal is aborted BEFORE making the fetch request
      const result = send({
        conversationId,
        content: contentToSend,
        signal: abortController.signal,
      });

      console.log("[FRONTEND] Step 5: Waiting for result.unwrap()");
      await result.unwrap();
      console.log("[FRONTEND] Step 6: Request completed successfully");

      // Only clear input if not aborted
      if (!abortController.signal.aborted) {
        setValue("");
      }
    } catch (err: any) {
      // Silently ignore abort/cancel errors
      if (
        err?.name === "AbortError" ||
        err?.name === "CanceledError" ||
        err?.status === 499 ||
        err?.data === "Cancelled" ||
        err?.data?.error === "Cancelled"
      ) {
        console.log(
          "[FRONTEND] Step 7: Request was aborted/cancelled - error name:",
          err?.name
        );
      } else {
        console.error("[FRONTEND] Step 7: Failed to send message:", err);
      }
    } finally {
      console.log(
        "[FRONTEND] Step 8: Cleanup - setting pending to false, clearing abortController"
      );
      isPendingRef.current = false;
      abortControllerRef.current = null;
    }
  };

  const handleCancel = () => {
    console.log("[FRONTEND] CANCEL CLICKED - handleCancel() called");
    // Abort the controller immediately - this prevents the fetch request
    if (abortControllerRef.current) {
      console.log(
        "[FRONTEND] CANCEL: Aborting controller, signal.aborted will be:",
        true
      );
      abortControllerRef.current.abort();
      console.log(
        "[FRONTEND] CANCEL: Controller aborted, signal.aborted is now:",
        abortControllerRef.current.signal.aborted
      );
      abortControllerRef.current = null;
    } else {
      console.log("[FRONTEND] CANCEL: No abortController found in ref");
    }
    isPendingRef.current = false;
    reset();
    console.log("[FRONTEND] CANCEL: Reset called, input should re-enable");
  };

  return (
    <Box p={2} borderTop={1} borderColor="divider">
      <Box display="flex" alignItems="flex-end" gap={1}>
        <TextField
          fullWidth
          multiline
          maxRows={5}
          value={value}
          onChange={(e) => setValue(e.target.value)}
          onKeyDown={(e) => {
            if (e.key === "Enter" && !e.shiftKey) {
              e.preventDefault();
              handleSend();
            }
          }}
          disabled={isLoading}
          placeholder="Type a message..."
        />
        {isLoading && <CircularProgress size={22} sx={{ mr: 0.5 }} />}
        <IconButton
          onClick={handleCancel}
          aria-label="Cancel sending"
          disabled={!isLoading}
          color="error"
        >
          <CancelIcon />
        </IconButton>
        <IconButton
          onClick={handleSend}
          aria-label="Send message"
          disabled={!value.trim() || isLoading}
          color="primary"
        >
          <SendIcon />
        </IconButton>
      </Box>
    </Box>
  );
}


--------------------------------------------------------------------------------

--- FILE: front\src\store\api.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\store\api.ts
# Size: 8886 bytes

import { createApi, fetchBaseQuery } from "@reduxjs/toolkit/query/react";
import type {
  Conversation,
  ConversationWithMessages,
  SendResponse,
} from "../types";

// Create baseQuery instance once for efficiency
const baseQueryInstance = fetchBaseQuery({
  baseUrl: "/api",
  timeout: 12000,
});

// Custom baseQuery that properly handles abort signals BEFORE and DURING the request
const customBaseQuery = async (args: any, api: any, extraOptions: any) => {
  console.log(
    "[RTK QUERY] Step A: customBaseQuery called, checking abort signal"
  );
  console.log("[RTK QUERY] signal.aborted =", args?.signal?.aborted);

  // CRITICAL: Check if signal is already aborted BEFORE making any request
  // This is the key check that prevents the request from reaching the backend
  if (args?.signal?.aborted) {
    console.log(
      "[RTK QUERY] Step B: Signal already aborted - throwing AbortError, fetch will NOT be called"
    );
    const error: any = new Error("Request was aborted");
    error.name = "AbortError";
    throw error;
  }

  console.log("[RTK QUERY] Step C: Signal not aborted - proceeding with fetch");
  console.log("[RTK QUERY] Step C: args keys =", Object.keys(args || {}));
  console.log("[RTK QUERY] Step C: args.signal exists =", !!args?.signal);

  // Ensure signal is explicitly passed to fetch
  // RTK Query's fetchBaseQuery extracts signal from the query result
  const modifiedArgs = {
    ...args,
    // Explicitly ensure signal is present if it exists
    ...(args?.signal && { signal: args.signal }),
  };

  console.log(
    "[RTK QUERY] Step C: modifiedArgs.signal exists =",
    !!modifiedArgs?.signal
  );

  // If signal is provided, wrap in Promise to immediately reject on abort
  if (args?.signal) {
    return new Promise<any>((resolve, reject) => {
      let isAborted = false;

      const abortHandler = () => {
        console.log(
          "[RTK QUERY] ABORT: Signal aborted during fetch - rejecting promise immediately"
        );
        isAborted = true;
        const error: any = new Error("Request was aborted");
        error.name = "AbortError";
        reject(error);
      };

      // Check if already aborted before setting up listener
      if (args.signal.aborted) {
        console.log("[RTK QUERY] ABORT: Signal already aborted before fetch");
        abortHandler();
        return;
      }

      // Set up abort listener BEFORE calling fetchBaseQuery
      args.signal.addEventListener("abort", abortHandler, { once: true });

      console.log(
        "[RTK QUERY] Step D: Added abort listener, calling fetchBaseQuery"
      );
      console.log(
        "[RTK QUERY] Step D: Signal will be passed to fetch - signal.aborted =",
        args.signal.aborted
      );

      // Call fetchBaseQuery but DON'T await it directly
      // If abort happens, we reject immediately and ignore the fetch result
      // NOTE: fetchBaseQuery should automatically extract signal from args and pass to fetch()
      Promise.resolve(baseQueryInstance(modifiedArgs, api, extraOptions))
        .then((result) => {
          // If we already rejected due to abort, ignore this result
          if (isAborted) {
            console.log(
              "[RTK QUERY] Step E: Fetch completed but was already aborted - ignoring result"
            );
            return;
          }

          // Remove listener since we're resolving
          args.signal?.removeEventListener("abort", abortHandler);

          // Final check: if aborted after fetch completed (race condition)
          if (args?.signal?.aborted) {
            console.log(
              "[RTK QUERY] Step E: Fetch completed but signal was aborted - rejecting"
            );
            const error: any = new Error("Request was aborted");
            error.name = "AbortError";
            reject(error);
            return;
          }

          console.log("[RTK QUERY] Step E: Fetch completed successfully");
          resolve(result);
        })
        .catch((err: any) => {
          // If we already rejected due to abort, ignore this error
          if (isAborted) {
            console.log(
              "[RTK QUERY] Step E: Fetch error but was already aborted - ignoring error"
            );
            return;
          }

          // Remove listener since we're rejecting
          args.signal?.removeEventListener("abort", abortHandler);

          // If it's an abort error, use our handler's error
          if (
            err?.name === "AbortError" ||
            err?.name === "CanceledError" ||
            args?.signal?.aborted
          ) {
            console.log(
              "[RTK QUERY] Step E: Fetch aborted - error:",
              err?.name,
              err?.message
            );
            const error: any = new Error("Request was aborted");
            error.name = "AbortError";
            reject(error);
            return;
          }

          console.log(
            "[RTK QUERY] Step E: Fetch failed - error:",
            err?.name,
            err?.message
          );
          reject(err);
        });
    });
  }

  // No signal provided - call normally
  console.log(
    "[RTK QUERY] Step D: No signal provided - calling fetchBaseQuery"
  );
  return baseQueryInstance(modifiedArgs, api, extraOptions);
};

export const api = createApi({
  baseQuery: customBaseQuery,
  tagTypes: ["Conversation"],
  endpoints: (builder) => ({
    getConversations: builder.query<Conversation[], void>({
      query: () => "/conversations",
      providesTags: (result) =>
        result
          ? [
              ...result.map(({ id }) => ({
                type: "Conversation" as const,
                id,
              })),
              { type: "Conversation", id: "LIST" },
            ]
          : [{ type: "Conversation", id: "LIST" }],
    }),

    createConversation: builder.mutation<Conversation, void>({
      query: () => ({
        url: "/conversations",
        method: "POST",
      }),
      invalidatesTags: [{ type: "Conversation", id: "LIST" }],
    }),

    getConversation: builder.query<
      ConversationWithMessages,
      { id: string; cursor?: string; limit?: number }
    >({
      query: ({ id, cursor, limit = 20 }) => ({
        url: `/conversations/${id}`,
        params: { messagesCursor: cursor, limit },
      }),
      providesTags: (result, error, { id }) => [{ type: "Conversation", id }],
    }),

    sendMessage: builder.mutation<
      SendResponse,
      { conversationId: string; content: string; signal?: AbortSignal }
    >({
      query: ({ conversationId, content, signal }) => ({
        url: `/conversations/${conversationId}/messages`,
        method: "POST",
        body: { content },
        signal, // Pass abort signal to fetch
      }),
      invalidatesTags: (result, error, { conversationId }) => [
        { type: "Conversation", id: conversationId },
        { type: "Conversation", id: "LIST" },
      ],
      async onQueryStarted(
        { conversationId, content },
        { dispatch, queryFulfilled }
      ) {
        // Optimistic update
        const patchResult = dispatch(
          api.util.updateQueryData(
            "getConversation",
            { id: conversationId },
            (draft) => {
              const tempId = `temp-${Date.now()}`;
              draft.messages.push({
                id: tempId,
                role: "user",
                content,
                createdAt: new Date().toISOString(),
              });
            }
          )
        );

        try {
          const { data } = await queryFulfilled;
          dispatch(
            api.util.updateQueryData(
              "getConversation",
              { id: conversationId },
              (draft) => {
                draft.messages = draft.messages.filter(
                  (m) => !m.id.startsWith("temp-")
                );
                draft.messages.push(data.message, data.reply);
              }
            )
          );
        } catch (err: any) {
          patchResult.undo();
        }
      },
    }),

    deleteConversation: builder.mutation<void, string>({
      query: (id) => ({
        url: `/conversations/${id}`,
        method: "DELETE",
      }),
      invalidatesTags: (result, error, id) => [
        { type: "Conversation", id },
        { type: "Conversation", id: "LIST" },
      ],
    }),
  }),
});

export const {
  useGetConversationsQuery,
  useCreateConversationMutation,
  useGetConversationQuery,
  useSendMessageMutation,
  useDeleteConversationMutation,
} = api;

// Note: Cancellation/abort handling has been removed for now


--------------------------------------------------------------------------------

--- FILE: front\src\store\hooks.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\store\hooks.ts
# Size: 322 bytes

import { useDispatch, useSelector, TypedUseSelectorHook } from 'react-redux';
import type { RootState, AppDispatch } from './index';

// Typed versions of useDispatch and useSelector
export const useAppDispatch = () => useDispatch<AppDispatch>();
export const useAppSelector: TypedUseSelectorHook<RootState> = useSelector;

--------------------------------------------------------------------------------

--- FILE: front\src\store\index.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\store\index.ts
# Size: 856 bytes

import { configureStore } from '@reduxjs/toolkit';
import { api } from './api';

// -------------------------------------------------
// 1. Create the store (only one declaration!)
export const store = configureStore({
  reducer: {
    [api.reducerPath]: api.reducer,
  },
  middleware: (getDefaultMiddleware) =>
    getDefaultMiddleware({
      serializableCheck: false,
    }).concat(api.middleware),
});

// -------------------------------------------------
// 2. Type exports (required for hooks)
export type RootState = ReturnType<typeof store.getState>;
export type AppDispatch = typeof store.dispatch;

// -------------------------------------------------
// 3. **DO NOT re-export `store` here** – it is already exported above.
//     Remove any line like: `export { store };` or `import store`
// -------------------------------------------------

--------------------------------------------------------------------------------

--- FILE: front\src\types\index.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\types\index.ts
# Size: 573 bytes

export type Role = 'user' | 'assistant';

export interface Message {
  id: string;
  role: Role;
  content: string;
  createdAt: string;
}

export interface Conversation {
  id: string;
  title: string;
  createdAt: string;
  lastMessageAt: string | null;
}

export interface PageInfo {
  nextCursor: string | null;
  prevCursor: string | null;
}

export interface ConversationWithMessages {
  id: string;
  title: string;
  messages: Message[];
  pageInfo: PageInfo;
}

export interface SendResponse {
  message: Message;
  reply: Message;
}

--------------------------------------------------------------------------------

--- FILE: backend\src\server.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\server.ts
# Size: 723 bytes

import express from 'express';
import cors from 'cors';
import conversationsRouter from './routes/conversations';
import messagesRouter from './routes/messages';
import { prisma } from './prisma';

const app = express();
app.use(cors());
app.use(express.json());

app.use('/api/conversations', conversationsRouter);
app.use('/api/conversations', messagesRouter);

app.get('/healthz', (req, res) => res.status(200).send('OK'));
app.get('/readyz', async (req, res) => {
  try {
    await prisma.$queryRaw`SELECT 1`;
    res.status(200).send('OK');
  } catch {
    res.status(500).send('DB not ready');
  }
});

const PORT = process.env.PORT || 3001;
app.listen(PORT, () => {
  console.log(`Backend listening on ${PORT}`);
});

--------------------------------------------------------------------------------

--- FILE: backend\src\adapters\factory.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\adapters\factory.ts
# Size: 644 bytes

import { MockAdapter } from './mockAdapter';
import { OllamaAdapter } from './ollamaAdapter';
import { LlmAdapter } from './llmAdapter';

export function createLlmAdapter(): LlmAdapter {
  const provider = process.env.LLM_PROVIDER || 'mock';

  if (provider === 'mock') {
    const url = process.env.MOCK_LLM_BASE_URL || 'http://mock-llm:8080';
    return new MockAdapter(url);
  }

  if (provider === 'ollama') {
    const url = process.env.OLLAMA_BASE_URL || 'http://ollama:11434';
    const model = process.env.OLLAMA_MODEL || 'llama3';
    return new OllamaAdapter(url, model);
  }

  throw new Error(`Unknown LLM_PROVIDER: ${provider}`);
}

--------------------------------------------------------------------------------

--- FILE: backend\src\adapters\llmAdapter.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\adapters\llmAdapter.ts
# Size: 178 bytes

export interface LlmAdapter {
  complete(
    messages: { role: "user" | "assistant"; content: string }[],
    signal?: AbortSignal
  ): Promise<{ completion: string }>;
}


--------------------------------------------------------------------------------

--- FILE: backend\src\adapters\mockAdapter.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\adapters\mockAdapter.ts
# Size: 568 bytes

import axios from "axios";
import { LlmAdapter } from "./llmAdapter";

export class MockAdapter implements LlmAdapter {
  constructor(private baseUrl: string) {}

  async complete(
    messages: { role: string; content: string }[],
    signal?: AbortSignal
  ) {
    const content = messages.map((m) => `${m.role}: ${m.content}`).join("\n");
    const res = await axios.post(
      `${this.baseUrl}/complete`,
      { content },
      {
        timeout: 12000,
        signal,
      }
    );
    return { completion: res.data.completion };
  }
}


--------------------------------------------------------------------------------

--- FILE: backend\src\adapters\ollamaAdapter.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\adapters\ollamaAdapter.ts
# Size: 604 bytes

import axios from "axios";
import { LlmAdapter } from "./llmAdapter";

export class OllamaAdapter implements LlmAdapter {
  constructor(private baseUrl: string, private model: string) {}

  async complete(
    messages: { role: "user" | "assistant"; content: string }[],
    signal?: AbortSignal
  ) {
    const res = await axios.post(
      `${this.baseUrl}/api/chat`,
      {
        model: this.model,
        messages,
        stream: false,
      },
      {
        timeout: 12000,
        signal,
      }
    );

    return { completion: res.data.message.content };
  }
}


--------------------------------------------------------------------------------

--- FILE: backend\src\prisma\index.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\prisma\index.ts
# Size: 328 bytes

import { PrismaClient } from '@prisma/client';

const globalForPrisma = global as unknown as { prisma: PrismaClient };

export const prisma =
  globalForPrisma.prisma ||
  new PrismaClient({
    log: ['query', 'info', 'warn', 'error'],
  });

if (process.env.NODE_ENV !== 'production') globalForPrisma.prisma = prisma;

--------------------------------------------------------------------------------

--- FILE: backend\src\routes\conversations.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\routes\conversations.ts
# Size: 953 bytes

import { Router } from 'express';
import { prisma } from '../prisma';
import { v4 as uuidv4 } from 'uuid';

const router = Router();

let convoCounter = 1;

// GET /api/conversations
router.get('/', async (req, res) => {
  const convos = await prisma.conversation.findMany({
    select: {
      id: true,
      title: true,
      createdAt: true,
      lastMessageAt: true,
    },
    orderBy: { createdAt: 'desc' },
  });
  res.json(convos);
});

// POST /api/conversations
router.post('/', async (req, res) => {
  const title = `Conversation #${convoCounter++}`;
  const convo = await prisma.conversation.create({
    data: { title },
  });
  res.status(201).json({
    id: convo.id,
    title: convo.title,
    createdAt: convo.createdAt,
  });
});

// DELETE /api/conversations/:id
router.delete('/:id', async (req, res) => {
  await prisma.conversation.delete({ where: { id: req.params.id } });
  res.status(204).send();
});

export default router;

--------------------------------------------------------------------------------

--- FILE: backend\src\routes\messages.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\routes\messages.ts
# Size: 10273 bytes

import { Router } from "express";
import { prisma } from "../prisma";
import { createLlmAdapter } from "../adapters/factory";
import { withRetry } from "../utils/retry";

const router = Router();
const llm = createLlmAdapter();

// GET /api/conversations/:id
router.get("/:id", async (req, res) => {
  const { id } = req.params;
  const cursor = req.query.messagesCursor as string | undefined;
  const limit = parseInt(req.query.limit as string) || 20;

  const convo = await prisma.conversation.findUnique({
    where: { id },
    select: { id: true, title: true },
  });

  if (!convo) return res.status(404).json({ error: "Not found" });

  const messages = await prisma.message.findMany({
    where: { conversationId: id },
    orderBy: [{ createdAt: "desc" }, { id: "desc" }],
    take: limit + 1,
    cursor: cursor ? { id: cursor } : undefined,
    skip: cursor ? 1 : 0,
  });

  const hasMore = messages.length > limit;
  const sliced = hasMore ? messages.slice(0, limit) : messages;
  const reversed = sliced.reverse();

  const prevCursor = reversed[0]?.id || null;
  const nextCursor = hasMore ? messages[messages.length - 1].id : null;

  res.json({
    ...convo,
    messages: reversed,
    pageInfo: { prevCursor, nextCursor },
  });
});

// POST /api/conversations/:id/messages
router.post("/:id/messages", async (req, res) => {
  const { content } = req.body;
  const { id } = req.params;

  console.log("[BACKEND] Step 1: Request received - POST /messages", { id, content: content.substring(0, 20) + "..." });

  // Preferred approach: Client aborts fetch, no server endpoint required
  // When the client aborts the fetch request, the connection closes and we detect it here
  // This cancels the in-flight LLM call automatically
  const abortController = new AbortController();
  let isClientConnected = true;
  let userMsg: any = null;
  let responseSent = false;

  console.log("[BACKEND] Step 2: AbortController created, signal.aborted =", abortController.signal.aborted);

  // Listen for client disconnect (happens when frontend aborts fetch)
  // Note: req.on("close") can fire prematurely, so we're careful about when we abort
  const cleanup = () => {
    console.log("[BACKEND] CLEANUP: req.aborted event fired!");
    console.log("[BACKEND] CLEANUP: isClientConnected =", isClientConnected, "responseSent =", responseSent);
    if (isClientConnected && !responseSent) {
      console.log("[BACKEND] CLEANUP: Setting isClientConnected = false and aborting controller");
      isClientConnected = false;
      abortController.abort(); // This will cancel the LLM call
      console.log("[BACKEND] CLEANUP: Controller aborted, signal.aborted =", abortController.signal.aborted);
      
      // If user message was created, delete it since request was cancelled
      if (userMsg && !responseSent) {
        console.log("[BACKEND] CLEANUP: Deleting user message from DB, id =", userMsg.id);
        prisma.message.delete({ where: { id: userMsg.id } }).catch(() => {
          // Ignore deletion errors
        });
      }
    } else {
      console.log("[BACKEND] CLEANUP: Skipping cleanup (already sent response or not connected)");
    }
  };

  // Only listen to 'aborted' event - more reliable than 'close'
  // 'close' fires too early for keep-alive connections
  req.on("aborted", cleanup);
  console.log("[BACKEND] Step 3: Registered req.on('aborted') listener");

  // Check if already aborted before saving to DB
  if (abortController.signal.aborted || !isClientConnected) {
    console.log("[BACKEND] Step 4: Already aborted before DB save - exiting");
    return; // Don't save anything if already cancelled
  }

  console.log("[BACKEND] Step 4: Saving user message to DB");
  userMsg = await prisma.message.create({
    data: { conversationId: id, role: "user", content },
  });
  console.log("[BACKEND] Step 5: User message saved, id =", userMsg.id);

  await prisma.conversation.update({
    where: { id },
    data: { lastMessageAt: new Date() },
  });

  try {
    // Check if cancelled before fetching history
    console.log("[BACKEND] Step 6: Checkpoint #1 - Before fetching history");
    console.log("[BACKEND] Step 6: signal.aborted =", abortController.signal.aborted, "isClientConnected =", isClientConnected);
    if (abortController.signal.aborted || !isClientConnected) {
      console.log("[BACKEND] Step 6: ABORTED at checkpoint #1 - deleting user message and exiting");
      if (userMsg) {
        await prisma.message.delete({ where: { id: userMsg.id } });
      }
      return;
    }

    // Fetch history
    console.log("[BACKEND] Step 7: Fetching conversation history");
    const dbHistory = await prisma.message.findMany({
      where: { conversationId: id },
      orderBy: { createdAt: "asc" },
      select: { role: true, content: true }, // Only send role + content
    });
    console.log("[BACKEND] Step 8: History fetched, messages count =", dbHistory.length);

    // Check again after DB fetch
    console.log("[BACKEND] Step 9: Checkpoint #2 - After fetching history");
    console.log("[BACKEND] Step 9: signal.aborted =", abortController.signal.aborted, "isClientConnected =", isClientConnected);
    if (abortController.signal.aborted || !isClientConnected) {
      console.log("[BACKEND] Step 9: ABORTED at checkpoint #2 - deleting user message and exiting");
      if (userMsg) {
        await prisma.message.delete({ where: { id: userMsg.id } });
      }
      return;
    }

    // Map to LLM format
    const llmHistory = dbHistory.map((m) => ({
      role: m.role as "user" | "assistant", // Type assertion (DB enforces)
      content: m.content,
    }));

    // Pass abort signal to LLM call - if client disconnects, this will cancel the LLM call
    console.log("[BACKEND] Step 10: Calling LLM with signal (signal.aborted =", abortController.signal.aborted, ")");
    const completion = await withRetry(
      () => llm.complete(llmHistory, abortController.signal),
      2,
      500,
      abortController.signal
    );
    console.log("[BACKEND] Step 11: LLM call completed");

    // Check if cancelled after LLM call
    console.log("[BACKEND] Step 12: Checkpoint #3 - After LLM call");
    console.log("[BACKEND] Step 12: signal.aborted =", abortController.signal.aborted, "isClientConnected =", isClientConnected);
    if (abortController.signal.aborted || !isClientConnected) {
      console.log("[BACKEND] Step 12: ABORTED at checkpoint #3 - deleting user message and exiting");
      if (userMsg) {
        await prisma.message.delete({ where: { id: userMsg.id } });
      }
      return;
    }

    console.log("[BACKEND] Step 13: Saving assistant message to DB");
    const assistantMsg = await prisma.message.create({
      data: {
        conversationId: id,
        role: "assistant",
        content: completion.completion,
      },
    });
    console.log("[BACKEND] Step 14: Assistant message saved, id =", assistantMsg.id);

    // Check if client is still connected before sending response
    console.log("[BACKEND] Step 15: Checkpoint #4 - Before sending response");
    console.log("[BACKEND] Step 15: signal.aborted =", abortController.signal.aborted, "isClientConnected =", isClientConnected);
    if (!isClientConnected || abortController.signal.aborted) {
      console.log("[BACKEND] Step 15: ABORTED at checkpoint #4 - not sending response");
      // Client disconnected/cancelled - don't send response
      return;
    }

    responseSent = true;
    console.log("[BACKEND] Step 16: Sending response to client");
    res.json({
      message: {
        id: userMsg.id,
        role: "user",
        content: userMsg.content,
        createdAt: userMsg.createdAt,
      },
      reply: {
        id: assistantMsg.id,
        role: "assistant",
        content: assistantMsg.content,
        createdAt: assistantMsg.createdAt,
      },
    });
    console.log("[BACKEND] Step 17: Response sent successfully");
  } catch (err: any) {
    console.log("[BACKEND] CATCH: Error caught in try-catch block");
    console.log("[BACKEND] CATCH: Error name =", err.name, "code =", err.code, "message =", err.message);
    
    // Clean up listeners
    req.removeListener("aborted", cleanup);

    // Ensure we always send a response if headers haven't been sent
    if (res.headersSent) {
      console.log("[BACKEND] CATCH: Headers already sent - returning");
      return;
    }

    responseSent = true;

    // Check if request was aborted/cancelled
    const isAborted = 
      abortController.signal.aborted ||
      err.name === "AbortError" ||
      err.name === "CanceledError" ||
      err.code === "ECONNABORTED" ||
      !isClientConnected;

    console.log("[BACKEND] CATCH: isAborted =", isAborted, "signal.aborted =", abortController.signal.aborted);
    
    if (isAborted) {
      console.log("[BACKEND] CATCH: Request was aborted/cancelled - cleaning up");
      // Client cancelled the request - clean up user message from DB
      if (userMsg) {
        console.log("[BACKEND] CATCH: Deleting user message, id =", userMsg.id);
        await prisma.message.delete({ where: { id: userMsg.id } }).catch(() => {
          // Ignore deletion errors
        });
      }
      // Don't send response for cancelled requests
      console.log("[BACKEND] CATCH: Not sending response for cancelled request");
      return;
    }

    // Check for timeout errors
    const isTimeout =
      err.code === "ETIMEDOUT" || err.message?.includes("timeout");

    if (isTimeout) {
      console.log("[BACKEND] CATCH: Timeout error - sending 500 response");
      res.status(500).json({ error: "LLM timeout", retryAfterMs: 1000 });
      return;
    }

    // All other errors (including LLM failures)
    console.error("[BACKEND] CATCH: Other error - sending 500 response", {
      errName: err.name,
      errMessage: err.message,
      errCode: err.code,
      headersSent: res.headersSent,
    });
    res.status(500).json({ error: "LLM failed", retryAfterMs: 1000 });
  }
});

export default router;


--------------------------------------------------------------------------------

--- FILE: backend\src\utils\retry.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\utils\retry.ts
# Size: 1496 bytes

export async function withRetry<T>(
  fn: () => Promise<T>,
  retries: number,
  delayMs: number,
  signal?: AbortSignal
): Promise<T> {
  let lastError;
  for (let i = 0; i <= retries; i++) {
    // Check if aborted before retry
    if (signal?.aborted) {
      const abortError = new Error("Aborted");
      abortError.name = "AbortError";
      throw abortError;
    }

    try {
      return await fn();
    } catch (err: any) {
      lastError = err;

      // Don't retry if aborted (axios throws CanceledError or AbortError)
      if (
        err.name === "AbortError" ||
        err.name === "CanceledError" ||
        signal?.aborted
      ) {
        const abortError = new Error("Aborted");
        abortError.name = "AbortError";
        throw abortError;
      }

      if (i < retries) {
        // Check abort during delay
        await new Promise((r, reject) => {
          const timeout = setTimeout(() => {
            signal?.removeEventListener("abort", abortHandler);
            r(undefined);
          }, delayMs * (i + 1));

          const abortHandler = () => {
            clearTimeout(timeout);
            signal?.removeEventListener("abort", abortHandler);
            const abortError = new Error("Aborted");
            abortError.name = "AbortError";
            reject(abortError);
          };

          signal?.addEventListener("abort", abortHandler);
        });
      }
    }
  }
  throw lastError;
}


--------------------------------------------------------------------------------


# TOTAL FILES COPIED: 39
# Finished at: 2025-10-31 17:13:56


--------------------------------------------------------------------------------

--- FILE: DECISIONS.md ---
# Path: C:\vs code projects\mini ChatGpt\DECISIONS.md
# Size: 607 bytes

# DECISIONS.md

## DB: PostgreSQL + Prisma
- ACID, relations, cursor pagination
- `prisma migrate deploy` in Docker
- Type-safe queries

## Backend: Node.js + Express + TypeScript
- Shared types with frontend
- Lightweight Docker image
- Full TS stack

## LLM Adapter
- `LLM_PROVIDER=mock|ollama`
- No code changes to switch
- Retry (2x), timeout (12s), cancel

## Pagination
- Cursor-based on `(createdAt DESC, id DESC)`
- Stable, no duplicates

## Resilience
- 500 → retry with backoff
- Hang → 12s timeout
- Cancel → aborts fetch

## Health Checks
- `/healthz`, `/readyz`

--------------------------------------------------------------------------------

--- FILE: docker-compose.yml ---
# Path: C:\vs code projects\mini ChatGpt\docker-compose.yml
# Size: 3827 bytes

services:
  # === DATABASE: PostgreSQL (Persistent) ===
  db:
    image: postgres:16
    container_name: chatgpt-db
    environment:
      POSTGRES_DB: chatgpt
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s

  # === MOCK LLM (Provided) ===
  mock-llm:
    build: ./mock-llm
    container_name: mock-llm
    ports:
      - "8080:8080"
    # Remove healthcheck — curl not in node:20-alpine
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8080/complete"]

  # === OLLAMA: Local LLM Service ===
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      # Increase timeout for model processing
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_LOAD_TIMEOUT=10m
      # Allow more time for model loading
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      # Debug logging to see what's happening
      - OLLAMA_DEBUG=true
    # Add resource limits if needed (optional - comment out if you want unlimited)
    # deploy:
    #   resources:
    #     limits:
    #       memory: 8G
    #     reservations:
    #       memory: 4G

  # === OLLAMA INIT: Pull model automatically (only if missing) ===
  ollama-init:
    image: curlimages/curl:latest
    container_name: ollama-init
    depends_on:
      ollama:
        condition: service_started
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for Ollama to be ready..."
        until curl -f http://ollama:11434/api/tags 2>/dev/null; do
          echo "Waiting for Ollama..."
          sleep 2
        done
        echo "Ollama is ready. Checking if model smollm2:135m exists..."
        MODELS=$$(curl -s http://ollama:11434/api/tags)
        if echo "$$MODELS" | grep -q '"name":"smollm2:135m"'; then
          echo "Model smollm2:135m already exists - no download needed."
          exit 0
        fi
        echo "Model smollm2:135m not found. Pulling (this may take several minutes)..."
        echo "NOTE: This only happens once - the model is saved and reused on future runs."
        echo "smollm2:135m is the LIGHTEST model (~200MB) - perfect for low-resource systems!"
        curl -X POST http://ollama:11434/api/pull -d '{"name":"smollm2:135m"}' --no-buffer
        echo "Model smollm2:135m pulled successfully!"

  # === BACKEND: Node.js + Express + Prisma ===
  backend:
    build: ./backend
    container_name: chatgpt-backend
    ports:
      - "3001:3001"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/chatgpt?schema=public
      - LLM_PROVIDER=ollama
      - MOCK_LLM_BASE_URL=http://mock-llm:8080
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=smollm2:135m
    depends_on:
      db:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully
      # Remove mock-llm dependency
      # mock-llm:
      #   condition: service_healthy
    command: >
      sh -c "
        echo 'Waiting for DB...' &&
        until nc -z db 5432; do sleep 1; done &&
        echo 'DB ready. Running migrations...' &&
        npx prisma migrate deploy &&
        echo 'Starting backend...' &&
        npm start
      "

  # === FRONTEND: React + MUI + Nginx ===
  frontend:
    build: ./front
    container_name: chatgpt-frontend
    ports:
      - "3000:80"
    depends_on:
      - backend

volumes:
  postgres_data:
  ollama_data:


--------------------------------------------------------------------------------

--- FILE: backend\package.json ---
# Path: C:\vs code projects\mini ChatGpt\backend\package.json
# Size: 671 bytes

{
  "name": "backend",
  "version": "1.0.0",
  "main": "dist/server.js",
  "scripts": {
    "build": "tsc",
    "start": "node dist/server.js",
    "dev": "ts-node src/server.ts",
    "migrate": "prisma migrate deploy"
  },
  "dependencies": {
    "@prisma/client": "^5.19.0",
    "axios": "^1.7.7",
    "cors": "^2.8.5",
    "express": "^4.19.2",
    "express-async-errors": "^3.1.1",
    "uuid": "^10.0.0"
  },
  "devDependencies": {
    "@types/cors": "^2.8.17",
    "@types/express": "^4.17.21",
    "@types/node": "^20.16.5",
    "@types/uuid": "^10.0.0",
    "prisma": "^5.19.0",
    "ts-node": "^10.9.2",
    "typescript": "^5.6.2"
  }
}

--------------------------------------------------------------------------------

--- FILE: backend\tsconfig.json ---
# Path: C:\vs code projects\mini ChatGpt\backend\tsconfig.json
# Size: 342 bytes

{
  "compilerOptions": {
    "target": "es2020",
    "module": "commonjs",
    "outDir": "dist",
    "rootDir": "src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true
  },
  "include": ["src"],
  "exclude": ["node_modules"]
}

--------------------------------------------------------------------------------

--- FILE: front\package.json ---
# Path: C:\vs code projects\mini ChatGpt\front\package.json
# Size: 1203 bytes

{
  "name": "front",
  "version": "0.1.0",
  "private": true,
  "dependencies": {
    "@emotion/react": "^11.14.0",
    "@emotion/styled": "^11.14.1",
    "@mui/icons-material": "^7.3.4",
    "@mui/material": "^7.3.4",
    "@reduxjs/toolkit": "^2.9.2",
    "@testing-library/dom": "^10.4.1",
    "@testing-library/jest-dom": "^6.9.1",
    "@testing-library/react": "^16.3.0",
    "@testing-library/user-event": "^13.5.0",
    "@types/jest": "^27.5.2",
    "@types/node": "^16.18.126",
    "@types/react": "^19.2.2",
    "@types/react-dom": "^19.2.2",
    "react": "^19.2.0",
    "react-dom": "^19.2.0",
    "react-redux": "^9.2.0",
    "react-scripts": "5.0.1",
    "typescript": "^4.9.5",
    "web-vitals": "^2.1.4"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test",
    "eject": "react-scripts eject"
  },
  "eslintConfig": {
    "extends": [
      "react-app",
      "react-app/jest"
    ]
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  }
}


--------------------------------------------------------------------------------

--- FILE: front\README.md ---
# Path: C:\vs code projects\mini ChatGpt\front\README.md
# Size: 2117 bytes

# Getting Started with Create React App

This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).

## Available Scripts

In the project directory, you can run:

### `npm start`

Runs the app in the development mode.\
Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

The page will reload if you make edits.\
You will also see any lint errors in the console.

### `npm test`

Launches the test runner in the interactive watch mode.\
See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.

### `npm run build`

Builds the app for production to the `build` folder.\
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.\
Your app is ready to be deployed!

See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.

### `npm run eject`

**Note: this is a one-way operation. Once you `eject`, you can’t go back!**

If you aren’t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.

Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.

You don’t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.

## Learn More

You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).

To learn React, check out the [React documentation](https://reactjs.org/).


--------------------------------------------------------------------------------

--- FILE: front\tsconfig.json ---
# Path: C:\vs code projects\mini ChatGpt\front\tsconfig.json
# Size: 682 bytes

{
  "compilerOptions": {
    "target": "es5",
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "noFallthroughCasesInSwitch": true,
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "baseUrl": "src",
    "paths": {
      "@/*": ["*"]
    }
  },
  "include": [
    "src"
  ],
  "exclude": [
    "node_modules",
    "build"
  ]
}

--------------------------------------------------------------------------------

--- FILE: mock-llm\package.json ---
# Path: C:\vs code projects\mini ChatGpt\mock-llm\package.json
# Size: 201 bytes

{
  "name": "mock-llm",
  "version": "1.0.0",
  "main": "server.js",
  "license": "MIT",
  "type": "commonjs",
  "dependencies": {
    "body-parser": "^1.20.2",
    "express": "^4.19.2"
  }
}

--------------------------------------------------------------------------------

--- FILE: mock-llm\server.js ---
# Path: C:\vs code projects\mini ChatGpt\mock-llm\server.js
# Size: 811 bytes

const express = require("express");
const bodyParser = require("body-parser");
const app = express();
app.use(bodyParser.json());

function randomInt(n) {
  return Math.floor(Math.random() * n);
}

app.post("/complete", async (req, res) => {
  if (Math.random() < 0.10) return; // hang forever
  if (Math.random() < 0.20) return res.status(500).json({ error: "mock-llm error" });

  const content = (req.body && req.body.content) || "";
  console.log("Mock LLM got:", content);

  const reply = "This is a mock response from a pretend LLM.";
  const delayMs = 500 + randomInt(1500);
  await new Promise(r => setTimeout(r, delayMs));

  return res.json({ completion: reply });
});

const port = process.env.PORT || 8080;
app.listen(port, () => console.log("mock-llm listening on", port));

--------------------------------------------------------------------------------

--- FILE: front\public\index.html ---
# Path: C:\vs code projects\mini ChatGpt\front\public\index.html
# Size: 1721 bytes

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Web site created using create-react-app"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>React App</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>


--------------------------------------------------------------------------------

--- FILE: front\public\manifest.json ---
# Path: C:\vs code projects\mini ChatGpt\front\public\manifest.json
# Size: 492 bytes

{
  "short_name": "React App",
  "name": "Create React App Sample",
  "icons": [
    {
      "src": "favicon.ico",
      "sizes": "64x64 32x32 24x24 16x16",
      "type": "image/x-icon"
    },
    {
      "src": "logo192.png",
      "type": "image/png",
      "sizes": "192x192"
    },
    {
      "src": "logo512.png",
      "type": "image/png",
      "sizes": "512x512"
    }
  ],
  "start_url": ".",
  "display": "standalone",
  "theme_color": "#000000",
  "background_color": "#ffffff"
}


--------------------------------------------------------------------------------

--- FILE: front\public\robots.txt ---
# Path: C:\vs code projects\mini ChatGpt\front\public\robots.txt
# Size: 67 bytes

# https://www.robotstxt.org/robotstxt.html
User-agent: *
Disallow:


--------------------------------------------------------------------------------

--- FILE: front\src\App.css ---
# Path: C:\vs code projects\mini ChatGpt\front\src\App.css
# Size: 564 bytes

.App {
  text-align: center;
}

.App-logo {
  height: 40vmin;
  pointer-events: none;
}

@media (prefers-reduced-motion: no-preference) {
  .App-logo {
    animation: App-logo-spin infinite 20s linear;
  }
}

.App-header {
  background-color: #282c34;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  font-size: calc(10px + 2vmin);
  color: white;
}

.App-link {
  color: #61dafb;
}

@keyframes App-logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}


--------------------------------------------------------------------------------

--- FILE: front\src\App.test.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\App.test.tsx
# Size: 273 bytes

import React from 'react';
import { render, screen } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  render(<App />);
  const linkElement = screen.getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});


--------------------------------------------------------------------------------

--- FILE: front\src\App.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\App.tsx
# Size: 939 bytes

import { Container, CssBaseline, AppBar, Toolbar, Typography, Box } from '@mui/material';
import ConversationList from './components/ConversationList';
import ChatWindow from './components/ChatWindow';
import { ThemeProvider } from '@mui/material/styles';
import theme from './theme';
import { useState } from 'react';

function App() {
  const [selectedId, setSelectedId] = useState<string | null>(null);

  return (
    <ThemeProvider theme={theme}>
      <CssBaseline />
      <AppBar position="static">
        <Toolbar>
          <Typography variant="h6">Mini ChatGPT</Typography>
        </Toolbar>
      </AppBar>

      <Container maxWidth={false} disableGutters sx={{ display: 'flex', height: 'calc(100vh - 64px)' }}>
        <ConversationList onSelect={setSelectedId} />
        <Box flex={1}>
          <ChatWindow conversationId={selectedId} />
        </Box>
      </Container>
    </ThemeProvider>
  );
}

export default App;

--------------------------------------------------------------------------------

--- FILE: front\src\index.css ---
# Path: C:\vs code projects\mini ChatGpt\front\src\index.css
# Size: 366 bytes

body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}


--------------------------------------------------------------------------------

--- FILE: front\src\index.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\index.tsx
# Size: 370 bytes

import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';
import { Provider } from 'react-redux';
import { store } from './store';

const root = ReactDOM.createRoot(
  document.getElementById('root') as HTMLElement
);
root.render(
  <React.StrictMode>
    <Provider store={store}>
      <App />
    </Provider>
  </React.StrictMode>
);

--------------------------------------------------------------------------------

--- FILE: front\src\react-app-env.d.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\react-app-env.d.ts
# Size: 41 bytes

/// <reference types="react-scripts" />


--------------------------------------------------------------------------------

--- FILE: front\src\reportWebVitals.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\reportWebVitals.ts
# Size: 425 bytes

import { ReportHandler } from 'web-vitals';

const reportWebVitals = (onPerfEntry?: ReportHandler) => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;


--------------------------------------------------------------------------------

--- FILE: front\src\setupTests.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\setupTests.ts
# Size: 241 bytes

// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';


--------------------------------------------------------------------------------

--- FILE: front\src\theme.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\theme.ts
# Size: 203 bytes

import { createTheme } from '@mui/material/styles';

export default createTheme({
  palette: {
    mode: 'light',
    primary: {
      main: '#1976d2',  // ← Fixed: 'd' not 'i'
    },
  },
});

--------------------------------------------------------------------------------

--- FILE: front\src\components\ChatWindow.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\components\ChatWindow.tsx
# Size: 3165 bytes

import { Box, CircularProgress, Button, Stack, Typography } from '@mui/material';
import { useGetConversationQuery, api } from '../store/api';
import { useAppDispatch } from '../store/hooks';
import MessageBubble from './MessageBubble';
import MessageInput from './MessageInput';
import { useEffect, useRef, useState } from 'react';
import type { Message } from '../types';

interface Props {
  conversationId: string | null;
}

export default function ChatWindow({ conversationId }: Props) {
  const dispatch = useAppDispatch();
  const [olderMessages, setOlderMessages] = useState<Message[]>([]);
  const [isLoadingOlder, setIsLoadingOlder] = useState(false);

  const {
    data: convo,
    isLoading,
    isFetching,
    error,
  } = useGetConversationQuery(
    { id: conversationId!, limit: 20 },
    { skip: !conversationId, refetchOnMountOrArgChange: true }
  );

  const scrollRef = useRef<HTMLDivElement>(null);

  // Clear older messages when switching conversation
  useEffect(() => {
    setOlderMessages([]);
  }, [conversationId]);

  useEffect(() => {
    scrollRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [convo?.messages, olderMessages]);

  const handleLoadMore = async () => {
    if (!convo?.pageInfo.prevCursor || !conversationId || isLoadingOlder) return;

    setIsLoadingOlder(true);
    try {
      const result = await dispatch(
        api.endpoints.getConversation.initiate({
          id: conversationId,
          cursor: convo.pageInfo.prevCursor,
          limit: 20,
        })
      ).unwrap();

      setOlderMessages((prev) => [...result.messages, ...prev]);
    } catch (err) {
      console.error('Failed to load older messages', err);
    } finally {
      setIsLoadingOlder(false);
    }
  };

  if (!conversationId) {
    return (
      <Box flex={1} display="flex" alignItems="center" justifyContent="center">
        <Typography color="text.secondary">Select or create a conversation</Typography>
      </Box>
    );
  }

  if (isLoading) {
    return (
      <Box flex={1} display="flex" alignItems="center" justifyContent="center">
        <CircularProgress />
      </Box>
    );
  }

  if (error) {
    return (
      <Box flex={1} p={3}>
        <Typography color="error">
          Failed to load chat: {(error as any).data || 'Try again'}
        </Typography>
      </Box>
    );
  }

  const allMessages = [...olderMessages, ...(convo?.messages || [])];

  return (
    <Box display="flex" flexDirection="column" height="100%">
      {/* Load Older Button */}
      {convo?.pageInfo.prevCursor && (
        <Stack alignItems="center" py={1}>
          <Button
            size="small"
            onClick={handleLoadMore}
            disabled={isLoadingOlder || isFetching}
          >
            {isLoadingOlder ? 'Loading...' : 'Load older messages'}
          </Button>
        </Stack>
      )}

      {/* Messages */}
      <Box flex={1} overflow="auto" p={2}>
        {allMessages.map((m) => (
          <MessageBubble key={m.id} msg={m} />
        ))}
        <div ref={scrollRef} />
      </Box>

      {/* Input */}
      <MessageInput conversationId={conversationId} />
    </Box>
  );
}

--------------------------------------------------------------------------------

--- FILE: front\src\components\ConversationList.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\components\ConversationList.tsx
# Size: 7274 bytes

import {
  List,
  ListItem,
  ListItemText,
  IconButton,
  Drawer,
  Box,
  Typography,
  Button,
  useMediaQuery,
  useTheme,
  Snackbar,
  Alert,
  ListItemButton,
} from "@mui/material";
import DeleteIcon from "@mui/icons-material/Delete";
import AddIcon from "@mui/icons-material/Add";
import MenuIcon from "@mui/icons-material/Menu";
import {
  useGetConversationsQuery,
  useCreateConversationMutation,
  useDeleteConversationMutation,
  api,
} from "../store/api";
import { useState, useEffect } from "react";
import { useAppDispatch } from "../store/hooks";

interface Props {
  onSelect: (id: string) => void;
}

export default function ConversationList({ onSelect }: Props) {
  const theme = useTheme();
  const isMobile = useMediaQuery(theme.breakpoints.down("md"));
  const [mobileOpen, setMobileOpen] = useState(false);
  const [undoId, setUndoId] = useState<string | null>(null);
  const [undoTimer, setUndoTimer] = useState<NodeJS.Timeout | null>(null);
  const [undoPatchResult, setUndoPatchResult] = useState<any>(null);

  const dispatch = useAppDispatch();
  const {
    data: conversations = [],
    isLoading,
    error,
    refetch,
  } = useGetConversationsQuery();

  // Debug logging
  useEffect(() => {
    console.log("[ConversationList] RTK Query state:", {
      conversations,
      conversationsLength: conversations.length,
      isLoading,
      error: error
        ? {
            status: (error as any)?.status,
            data: (error as any)?.data,
            message: (error as any)?.message,
          }
        : null,
    });
  }, [conversations, isLoading, error]);
  const [create] = useCreateConversationMutation();
  const [deleteConvo] = useDeleteConversationMutation();

  const handleDelete = (id: string) => {
    // Optimistic update - remove from cache immediately
    const patchResult = dispatch(
      api.util.updateQueryData("getConversations", undefined, (draft) => {
        if (!draft) return [];
        return draft.filter((c) => c.id !== id);
      })
    );
    setUndoPatchResult(patchResult);

    // Schedule actual delete after 5 seconds (for undo functionality)
    const timer = setTimeout(async () => {
      try {
        await deleteConvo(id).unwrap();
        // RTK Query automatically refetches due to invalidatesTags
        setUndoId(null);
        setUndoPatchResult(null);
      } catch (err) {
        console.error("Delete failed:", err);
        // Restore on error
        patchResult.undo();
        setUndoId(null);
        setUndoPatchResult(null);
      }
    }, 5000);

    setUndoId(id);
    setUndoTimer(timer);
  };

  const handleUndo = () => {
    if (undoTimer) clearTimeout(undoTimer);
    // Restore the optimistic update
    if (undoPatchResult) {
      undoPatchResult.undo();
      setUndoPatchResult(null);
    }
    setUndoId(null);
  };

  useEffect(() => {
    return () => {
      if (undoTimer) clearTimeout(undoTimer);
    };
  }, [undoTimer]);

  return (
    <>
      {/* Desktop Sidebar */}
      {!isMobile && (
        <Box
          sx={{
            width: 300,
            borderRight: 1,
            borderColor: "divider",
            overflow: "auto",
          }}
        >
          <InnerList />
        </Box>
      )}

      {/* Mobile Drawer */}
      {isMobile && (
        <>
          <IconButton onClick={() => setMobileOpen(true)} sx={{ ml: 1, mt: 1 }}>
            <MenuIcon />
          </IconButton>
          <Drawer open={mobileOpen} onClose={() => setMobileOpen(false)}>
            <Box sx={{ width: 280 }}>
              <InnerList onClose={() => setMobileOpen(false)} />
            </Box>
          </Drawer>
        </>
      )}

      {/* Undo Snackbar */}
      <Snackbar
        open={!!undoId}
        anchorOrigin={{ vertical: "bottom", horizontal: "center" }}
        autoHideDuration={null}
      >
        <Alert
          severity="info"
          action={
            <Button color="inherit" size="small" onClick={handleUndo}>
              UNDO
            </Button>
          }
        >
          Conversation deleted
        </Alert>
      </Snackbar>
    </>
  );

  function InnerList({ onClose }: { onClose?: () => void }) {
    return (
      <>
        <Box p={2} display="flex" justifyContent="space-between">
          <Typography variant="h6">Chats</Typography>
          <Button
            startIcon={<AddIcon />}
            onClick={async () => {
              try {
                const res = await create().unwrap();
                // RTK Query automatically refetches due to invalidatesTags
                onSelect(res.id);
                onClose?.();
              } catch (err) {
                console.error("Create failed", err);
              }
            }}
          >
            New
          </Button>
        </Box>

        {error ? (
          <Box p={2}>
            <Typography color="error" variant="body2" sx={{ mb: 2 }}>
              Failed to load conversations:{" "}
              {(error as any)?.data?.message ||
                (error as any)?.message ||
                "Unknown error"}
            </Typography>
            <Button
              onClick={() => refetch()}
              variant="outlined"
              size="small"
              fullWidth
            >
              Retry
            </Button>
          </Box>
        ) : isLoading ? (
          <Typography p={2}>Loading...</Typography>
        ) : conversations.length > 0 ? (
          <List>
            {conversations.map((c) => (
              <ListItem key={c.id} disablePadding>
                <ListItemButton
                  onClick={() => {
                    onSelect(c.id);
                    onClose?.();
                  }}
                  sx={{ pl: 2, pr: 1 }}
                >
                  <ListItemText
                    primary={c.title}
                    secondary={
                      c.lastMessageAt
                        ? new Date(c.lastMessageAt).toLocaleString()
                        : "No messages"
                    }
                  />
                  <IconButton
                    edge="end"
                    onClick={(e) => {
                      e.stopPropagation();
                      handleDelete(c.id);
                    }}
                  >
                    <DeleteIcon />
                  </IconButton>
                </ListItemButton>
              </ListItem>
            ))}
          </List>
        ) : (
          <Box p={2} textAlign="center">
            <Typography variant="body2" color="text.secondary" sx={{ mb: 2 }}>
              No conversations yet
            </Typography>
            <Button
              variant="outlined"
              size="small"
              startIcon={<AddIcon />}
              onClick={async () => {
                try {
                  const res = await create().unwrap();
                  // RTK Query automatically refetches due to invalidatesTags
                  onSelect(res.id);
                  onClose?.();
                } catch (err) {
                  console.error("Create failed", err);
                }
              }}
            >
              Create your first chat
            </Button>
          </Box>
        )}
      </>
    );
  }
}


--------------------------------------------------------------------------------

--- FILE: front\src\components\MessageBubble.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\components\MessageBubble.tsx
# Size: 691 bytes

import { Paper, Typography, Box } from '@mui/material';
import { Message } from '../types';

export default function MessageBubble({ msg }: { msg: Message }) {
  const isUser = msg.role === 'user';
  return (
    <Box sx={{ display: 'flex', justifyContent: isUser ? 'flex-end' : 'flex-start', mb: 1.5 }}>
      <Paper
        sx={{
          maxWidth: '75%',
          p: 1.5,
          bgcolor: isUser ? 'primary.main' : 'grey.100',
          color: isUser ? 'white' : 'text.primary',
          borderRadius: 2,
        }}
      >
        <Typography variant="body1" whiteSpace="pre-wrap">
          {msg.content}
        </Typography>
      </Paper>
    </Box>
  );
}

--------------------------------------------------------------------------------

--- FILE: front\src\components\MessageInput.tsx ---
# Path: C:\vs code projects\mini ChatGpt\front\src\components\MessageInput.tsx
# Size: 3271 bytes

import { TextField, IconButton, Box, CircularProgress } from "@mui/material";
import SendIcon from "@mui/icons-material/Send";
import CancelIcon from "@mui/icons-material/Cancel";
import { useSendMessageMutation } from "../store/api";
import { useRef, useState } from "react";

interface Props {
  conversationId: string;
}

export default function MessageInput({ conversationId }: Props) {
  const [value, setValue] = useState("");
  const [sendMessage, { isLoading }] = useSendMessageMutation();
  const abortControllerRef = useRef<AbortController | null>(null);
  const isPendingRef = useRef(false);

  const handleSend = async () => {
    if (!value.trim() || isLoading || isPendingRef.current) return;

    // Mark as pending to prevent duplicate sends
    isPendingRef.current = true;

    // Create a new AbortController for this request
    const abortController = new AbortController();
    abortControllerRef.current = abortController;

    const contentToSend = value.trim();

    try {
      // RTK Query will handle the abort signal automatically
      // The sendMessage endpoint checks if signal is aborted before making the request
      await sendMessage({
        conversationId,
        content: contentToSend,
        signal: abortController.signal,
      }).unwrap();

      // Only clear input if not aborted
      if (!abortController.signal.aborted) {
        setValue("");
      }
    } catch (err: any) {
      // Silently ignore abort errors
      const isAbortError =
        err?.name === "AbortError" ||
        err?.name === "CanceledError" ||
        err?.message === "Request was aborted" ||
        (err instanceof DOMException && err.name === "AbortError");

      if (!isAbortError) {
        console.error("[FRONTEND] Failed to send message:", err);
        // Could show error toast here
      }
    } finally {
      isPendingRef.current = false;
      abortControllerRef.current = null;
    }
  };

  const handleCancel = () => {
    // Abort the controller - RTK Query will handle aborting the fetch request
    if (abortControllerRef.current) {
      abortControllerRef.current.abort();
      abortControllerRef.current = null;
    }
    isPendingRef.current = false;
  };

  return (
    <Box p={2} borderTop={1} borderColor="divider">
      <Box display="flex" alignItems="flex-end" gap={1}>
        <TextField
          fullWidth
          multiline
          maxRows={5}
          value={value}
          onChange={(e) => setValue(e.target.value)}
          onKeyDown={(e) => {
            if (e.key === "Enter" && !e.shiftKey) {
              e.preventDefault();
              handleSend();
            }
          }}
          disabled={isLoading}
          placeholder="Type a message..."
        />
        {isLoading && <CircularProgress size={22} sx={{ mr: 0.5 }} />}
        <IconButton
          onClick={handleCancel}
          aria-label="Cancel sending"
          disabled={!isLoading}
          color="error"
        >
          <CancelIcon />
        </IconButton>
        <IconButton
          onClick={handleSend}
          aria-label="Send message"
          disabled={!value.trim() || isLoading}
          color="primary"
        >
          <SendIcon />
        </IconButton>
      </Box>
    </Box>
  );
}


--------------------------------------------------------------------------------

--- FILE: front\src\store\api.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\store\api.ts
# Size: 5709 bytes

import { createApi, fetchBaseQuery } from "@reduxjs/toolkit/query/react";
import type {
  Conversation,
  ConversationWithMessages,
  SendResponse,
} from "../types";

export const api = createApi({
  baseQuery: fetchBaseQuery({
    baseUrl: "/api",
    timeout: 12000,
  }),
  tagTypes: ["Conversation"],
  endpoints: (builder) => ({
    getConversations: builder.query<Conversation[], void>({
      query: () => "/conversations",
      providesTags: (result) =>
        result
          ? [
              ...result.map(({ id }) => ({
                type: "Conversation" as const,
                id,
              })),
              { type: "Conversation", id: "LIST" },
            ]
          : [{ type: "Conversation", id: "LIST" }],
    }),

    createConversation: builder.mutation<Conversation, void>({
      query: () => ({
        url: "/conversations",
        method: "POST",
      }),
      invalidatesTags: [{ type: "Conversation", id: "LIST" }],
    }),

    getConversation: builder.query<
      ConversationWithMessages,
      { id: string; cursor?: string; limit?: number }
    >({
      query: ({ id, cursor, limit = 20 }) => ({
        url: `/conversations/${id}`,
        params: { messagesCursor: cursor, limit },
      }),
      providesTags: (result, error, { id }) => [{ type: "Conversation", id }],
    }),

    sendMessage: builder.mutation<
      SendResponse,
      { conversationId: string; content: string; signal?: AbortSignal }
    >({
      query: ({ conversationId, content, signal }) => ({
        url: `/conversations/${conversationId}/messages`,
        method: "POST",
        body: { content },
        signal, // RTK Query's fetchBaseQuery passes this to fetch() - fetch() will throw AbortError if already aborted
      }),
      invalidatesTags: (result, error, { conversationId }) => {
        // Only invalidate if the request succeeded (not on abort/error)
        if (error) {
          // Check if it's an abort error by examining the error structure
          // FetchBaseQueryError might have status 'FETCH_ERROR' for abort errors
          const isAbortError =
            error && typeof error === "object" && "status" in error
              ? error.status === "FETCH_ERROR" ||
                (error as any).error === "AbortError" ||
                (error as any).name === "AbortError"
              : false;

          // Don't invalidate on abort - the optimistic update will be reverted manually
          if (isAbortError) {
            return [];
          }
        }

        // Invalidate on success
        return [
          { type: "Conversation", id: conversationId },
          { type: "Conversation", id: "LIST" },
        ];
      },
      async onQueryStarted(
        { conversationId, content },
        { dispatch, queryFulfilled }
      ) {
        // Optimistic update - add user message to conversation
        const conversationPatchResult = dispatch(
          api.util.updateQueryData(
            "getConversation",
            { id: conversationId },
            (draft) => {
              if (!draft || !draft.messages) return;
              const tempId = `temp-${Date.now()}`;
              draft.messages.push({
                id: tempId,
                role: "user",
                content,
                createdAt: new Date().toISOString(),
              });
            }
          )
        );

        // Optimistic update - update lastMessageAt in conversations list
        const conversationsPatchResult = dispatch(
          api.util.updateQueryData("getConversations", undefined, (draft) => {
            if (!draft || !Array.isArray(draft)) return;
            const conversation = draft.find((c) => c.id === conversationId);
            if (conversation) {
              conversation.lastMessageAt = new Date().toISOString();
            }
          })
        );

        try {
          const { data } = await queryFulfilled;
          // Replace optimistic message with real messages
          dispatch(
            api.util.updateQueryData(
              "getConversation",
              { id: conversationId },
              (draft) => {
                if (!draft || !draft.messages) return;
                draft.messages = draft.messages.filter(
                  (m) => !m.id.startsWith("temp-")
                );
                draft.messages.push(data.message, data.reply);
              }
            )
          );
          // Conversations list will be refetched automatically via invalidatesTags
        } catch (err: any) {
          // Check if this is an abort error
          const isAbortError =
            err?.name === "AbortError" ||
            err?.name === "CanceledError" ||
            err?.message === "Request was aborted" ||
            (err instanceof DOMException && err.name === "AbortError");

          // Always undo optimistic updates on error (including abort)
          conversationPatchResult.undo();

          // For abort errors, also undo the conversations list update
          // For other errors, keep it (the message was sent but failed to get response)
          if (isAbortError) {
            conversationsPatchResult.undo();
          }
        }
      },
    }),

    deleteConversation: builder.mutation<void, string>({
      query: (id) => ({
        url: `/conversations/${id}`,
        method: "DELETE",
      }),
      invalidatesTags: (result, error, id) => [
        { type: "Conversation", id },
        { type: "Conversation", id: "LIST" },
      ],
    }),
  }),
});

export const {
  useGetConversationsQuery,
  useCreateConversationMutation,
  useGetConversationQuery,
  useSendMessageMutation,
  useDeleteConversationMutation,
} = api;


--------------------------------------------------------------------------------

--- FILE: front\src\store\hooks.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\store\hooks.ts
# Size: 322 bytes

import { useDispatch, useSelector, TypedUseSelectorHook } from 'react-redux';
import type { RootState, AppDispatch } from './index';

// Typed versions of useDispatch and useSelector
export const useAppDispatch = () => useDispatch<AppDispatch>();
export const useAppSelector: TypedUseSelectorHook<RootState> = useSelector;

--------------------------------------------------------------------------------

--- FILE: front\src\store\index.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\store\index.ts
# Size: 856 bytes

import { configureStore } from '@reduxjs/toolkit';
import { api } from './api';

// -------------------------------------------------
// 1. Create the store (only one declaration!)
export const store = configureStore({
  reducer: {
    [api.reducerPath]: api.reducer,
  },
  middleware: (getDefaultMiddleware) =>
    getDefaultMiddleware({
      serializableCheck: false,
    }).concat(api.middleware),
});

// -------------------------------------------------
// 2. Type exports (required for hooks)
export type RootState = ReturnType<typeof store.getState>;
export type AppDispatch = typeof store.dispatch;

// -------------------------------------------------
// 3. **DO NOT re-export `store` here** – it is already exported above.
//     Remove any line like: `export { store };` or `import store`
// -------------------------------------------------

--------------------------------------------------------------------------------

--- FILE: front\src\types\index.ts ---
# Path: C:\vs code projects\mini ChatGpt\front\src\types\index.ts
# Size: 573 bytes

export type Role = 'user' | 'assistant';

export interface Message {
  id: string;
  role: Role;
  content: string;
  createdAt: string;
}

export interface Conversation {
  id: string;
  title: string;
  createdAt: string;
  lastMessageAt: string | null;
}

export interface PageInfo {
  nextCursor: string | null;
  prevCursor: string | null;
}

export interface ConversationWithMessages {
  id: string;
  title: string;
  messages: Message[];
  pageInfo: PageInfo;
}

export interface SendResponse {
  message: Message;
  reply: Message;
}

--------------------------------------------------------------------------------

--- FILE: backend\src\server.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\server.ts
# Size: 1125 bytes

import express from "express";
import cors from "cors";
import conversationsRouter from "./routes/conversations";
import messagesRouter from "./routes/messages";
import { prisma } from "./prisma";

const app = express();
app.use(cors());
app.use(express.json());

// Add request logging middleware
app.use("/api/conversations", (req, res, next) => {
  console.log(`[SERVER] ${req.method} ${req.path} - ${req.url}`);
  next();
});

// Mount conversationsRouter first - it handles GET /, POST /, DELETE /:id
app.use("/api/conversations", conversationsRouter);
// Mount messagesRouter second - it handles GET /:id, POST /:id/messages
// This order matters: more specific routes (/:id) should come after general ones (/)
app.use("/api/conversations", messagesRouter);

app.get("/healthz", (req, res) => res.status(200).send("OK"));
app.get("/readyz", async (req, res) => {
  try {
    await prisma.$queryRaw`SELECT 1`;
    res.status(200).send("OK");
  } catch {
    res.status(500).send("DB not ready");
  }
});

const PORT = process.env.PORT || 3001;
app.listen(PORT, () => {
  console.log(`Backend listening on ${PORT}`);
});


--------------------------------------------------------------------------------

--- FILE: backend\src\adapters\factory.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\adapters\factory.ts
# Size: 900 bytes

import { MockAdapter } from './mockAdapter';
import { OllamaAdapter } from './ollamaAdapter';
import { LlmAdapter } from './llmAdapter';

export function createLlmAdapter(): LlmAdapter {
  const provider = process.env.LLM_PROVIDER || 'mock';
  console.log('[LLM FACTORY] Creating LLM adapter, provider =', provider);

  if (provider === 'mock') {
    const url = process.env.MOCK_LLM_BASE_URL || 'http://mock-llm:8080';
    console.log('[LLM FACTORY] Creating MockAdapter with URL:', url);
    return new MockAdapter(url);
  }

  if (provider === 'ollama') {
    const url = process.env.OLLAMA_BASE_URL || 'http://ollama:11434';
    const model = process.env.OLLAMA_MODEL || 'llama3';
    console.log('[LLM FACTORY] Creating OllamaAdapter with URL:', url, 'Model:', model);
    return new OllamaAdapter(url, model);
  }

  throw new Error(`Unknown LLM_PROVIDER: ${provider}`);
}

--------------------------------------------------------------------------------

--- FILE: backend\src\adapters\llmAdapter.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\adapters\llmAdapter.ts
# Size: 178 bytes

export interface LlmAdapter {
  complete(
    messages: { role: "user" | "assistant"; content: string }[],
    signal?: AbortSignal
  ): Promise<{ completion: string }>;
}


--------------------------------------------------------------------------------

--- FILE: backend\src\adapters\mockAdapter.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\adapters\mockAdapter.ts
# Size: 1751 bytes

import axios from "axios";
import { LlmAdapter } from "./llmAdapter";

export class MockAdapter implements LlmAdapter {
  constructor(private baseUrl: string) {}

  async complete(
    messages: { role: string; content: string }[],
    signal?: AbortSignal
  ) {
    console.log('[MOCK ADAPTER] Starting complete()');
    console.log('[MOCK ADAPTER] Base URL:', this.baseUrl);
    console.log('[MOCK ADAPTER] Messages count:', messages.length);
    console.log('[MOCK ADAPTER] Signal aborted:', signal?.aborted || false);
    
    const content = messages.map((m) => `${m.role}: ${m.content}`).join("\n");
    console.log('[MOCK ADAPTER] Request URL:', `${this.baseUrl}/complete`);
    console.log('[MOCK ADAPTER] Content length:', content.length);
    
    const startTime = Date.now();
    try {
      console.log('[MOCK ADAPTER] Making POST request to mock LLM...');
      const res = await axios.post(
        `${this.baseUrl}/complete`,
        { content },
        {
          timeout: 12000,
          signal,
        }
      );
      const duration = Date.now() - startTime;
      console.log('[MOCK ADAPTER] ✅ Request completed successfully in', duration, 'ms');
      console.log('[MOCK ADAPTER] Response status:', res.status);
      console.log('[MOCK ADAPTER] Completion length:', res.data?.completion?.length || 0);
      return { completion: res.data.completion };
    } catch (err: any) {
      const duration = Date.now() - startTime;
      console.error('[MOCK ADAPTER] ❌ Error occurred after', duration, 'ms');
      console.error('[MOCK ADAPTER] Error name:', err.name);
      console.error('[MOCK ADAPTER] Error message:', err.message);
      console.error('[MOCK ADAPTER] Error code:', err.code);
      throw err;
    }
  }
}


--------------------------------------------------------------------------------

--- FILE: backend\src\adapters\ollamaAdapter.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\adapters\ollamaAdapter.ts
# Size: 5006 bytes

import axios from "axios";
import { LlmAdapter } from "./llmAdapter";

export class OllamaAdapter implements LlmAdapter {
  constructor(private baseUrl: string, private model: string) {}

  async complete(
    messages: { role: "user" | "assistant"; content: string }[],
    signal?: AbortSignal
  ) {
    console.log('[OLLAMA ADAPTER] Starting complete()');
    console.log('[OLLAMA ADAPTER] Base URL:', this.baseUrl);
    console.log('[OLLAMA ADAPTER] Model:', this.model);
    console.log('[OLLAMA ADAPTER] Messages count:', messages.length);
    console.log('[OLLAMA ADAPTER] Signal aborted:', signal?.aborted || false);
    console.log('[OLLAMA ADAPTER] Request URL:', `${this.baseUrl}/api/chat`);

    const startTime = Date.now();
    
    try {
      console.log('[OLLAMA ADAPTER] Making POST request to Ollama...');
      const res = await axios.post(
        `${this.baseUrl}/api/chat`,
        {
          model: this.model,
          messages,
          stream: false,
        },
        {
          timeout: 120000, // 120s - allows time for model loading + generation, and to catch Ollama's actual error responses
          signal,
        }
      );

      const duration = Date.now() - startTime;
      console.log('[OLLAMA ADAPTER] ✅ Request completed successfully');
      console.log('[OLLAMA ADAPTER] Duration:', duration, 'ms');
      console.log('[OLLAMA ADAPTER] Response status:', res.status);
      console.log('[OLLAMA ADAPTER] Response data keys:', Object.keys(res.data || {}));
      console.log('[OLLAMA ADAPTER] Completion length:', res.data?.message?.content?.length || 0);

      return { completion: res.data.message.content };
    } catch (err: any) {
      const duration = Date.now() - startTime;
      console.error('[OLLAMA ADAPTER] ❌ Error occurred');
      console.error('[OLLAMA ADAPTER] Duration before error:', duration, 'ms');
      console.error('[OLLAMA ADAPTER] Error name:', err.name);
      console.error('[OLLAMA ADAPTER] Error message:', err.message);
      console.error('[OLLAMA ADAPTER] Error code:', err.code);
      console.error('[OLLAMA ADAPTER] Signal aborted:', signal?.aborted || false);
      
      if (err.response) {
        console.error('[OLLAMA ADAPTER] Response status:', err.response.status);
        console.error('[OLLAMA ADAPTER] Response data:', err.response.data);
      }
      
      if (err.request && !err.response) {
        console.error('[OLLAMA ADAPTER] Request was made but no response received (timeout or connection error)');
        console.error('[OLLAMA ADAPTER] Request config:', {
          url: err.config?.url,
          method: err.config?.method,
          timeout: err.config?.timeout,
        });
        // If timeout and no response, Ollama likely timed out internally
        if (err.code === 'ECONNABORTED' && err.message.includes('timeout')) {
          console.error('[OLLAMA ADAPTER] ⚠️ Ollama timed out after 60s - this may indicate:');
          console.error('[OLLAMA ADAPTER]   1. Model is still loading into memory');
          console.error('[OLLAMA ADAPTER]   2. Ollama is out of memory/CPU resources');
          console.error('[OLLAMA ADAPTER]   3. Model generation is taking longer than expected');
          throw new Error(`Ollama request timed out after 60 seconds. The model may still be loading or Ollama may need more resources.`);
        }
      }

      // Check for HTTP error responses from Ollama
      if (err.response) {
        const status = err.response.status;
        if (status === 404) {
          const errorMsg = `Ollama model "${this.model}" not found. Please wait for the model to finish downloading.`;
          console.error('[OLLAMA ADAPTER] Model not found error:', errorMsg);
          throw new Error(errorMsg);
        }
        if (status === 500) {
          const errorData = err.response.data || {};
          const errorText = typeof errorData === 'string' ? errorData : JSON.stringify(errorData);
          const errorMsg = `Ollama returned a 500 error: ${errorText}. This may indicate Ollama is out of resources, the model is still loading, or there's an internal error.`;
          console.error('[OLLAMA ADAPTER] ⚠️ Ollama 500 error response received!');
          console.error('[OLLAMA ADAPTER] Error data:', errorData);
          console.error('[OLLAMA ADAPTER] Error headers:', err.response.headers);
          console.error('[OLLAMA ADAPTER] Full error response:', JSON.stringify(err.response.data, null, 2));
          throw new Error(errorMsg);
        }
        // Other HTTP errors
        const errorMsg = `Ollama returned ${status}: ${JSON.stringify(err.response.data || {})}`;
        console.error('[OLLAMA ADAPTER] Ollama HTTP error:', errorMsg);
        throw new Error(errorMsg);
      }
      
      // Re-throw other errors as-is
      console.error('[OLLAMA ADAPTER] Re-throwing error');
      throw err;
    }
  }
}


--------------------------------------------------------------------------------

--- FILE: backend\src\prisma\index.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\prisma\index.ts
# Size: 328 bytes

import { PrismaClient } from '@prisma/client';

const globalForPrisma = global as unknown as { prisma: PrismaClient };

export const prisma =
  globalForPrisma.prisma ||
  new PrismaClient({
    log: ['query', 'info', 'warn', 'error'],
  });

if (process.env.NODE_ENV !== 'production') globalForPrisma.prisma = prisma;

--------------------------------------------------------------------------------

--- FILE: backend\src\routes\conversations.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\routes\conversations.ts
# Size: 1431 bytes

import { Router } from "express";
import { prisma } from "../prisma";
import { v4 as uuidv4 } from "uuid";

const router = Router();

let convoCounter = 1;

// GET /api/conversations
router.get("/", async (req, res) => {
  console.log("[GET /conversations] Route hit!");
  try {
    const convos = await prisma.conversation.findMany({
      select: {
        id: true,
        title: true,
        createdAt: true,
        lastMessageAt: true,
      },
      orderBy: { createdAt: "desc" },
    });
    console.log(
      "[GET /conversations] Found",
      convos.length,
      "conversations, returning status 200"
    );
    // Explicitly set status to 200 and ensure we always return an array
    res.status(200).json(convos || []);
  } catch (error) {
    console.error("[GET /conversations] Error:", error);
    res.status(500).json({ error: "Failed to fetch conversations" });
  }
});

// POST /api/conversations
router.post("/", async (req, res) => {
  const title = `Conversation #${convoCounter++}`;
  const convo = await prisma.conversation.create({
    data: { title },
    select: {
      id: true,
      title: true,
      createdAt: true,
      lastMessageAt: true,
    },
  });
  res.status(201).json(convo);
});

// DELETE /api/conversations/:id
router.delete("/:id", async (req, res) => {
  await prisma.conversation.delete({ where: { id: req.params.id } });
  res.status(204).send();
});

export default router;


--------------------------------------------------------------------------------

--- FILE: backend\src\routes\messages.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\routes\messages.ts
# Size: 14364 bytes

import { Router } from "express";
import { prisma } from "../prisma";
import { createLlmAdapter } from "../adapters/factory";
import { withRetry } from "../utils/retry";

const router = Router();
console.log("[MESSAGES ROUTE] Initializing messages router...");
const llm = createLlmAdapter();
console.log("[MESSAGES ROUTE] LLM adapter created:", llm.constructor.name);

// GET /api/conversations/:id
router.get("/:id", async (req, res) => {
  const { id } = req.params;
  const cursor = req.query.messagesCursor as string | undefined;
  const limit = parseInt(req.query.limit as string) || 20;

  const convo = await prisma.conversation.findUnique({
    where: { id },
    select: { id: true, title: true },
  });

  if (!convo) return res.status(404).json({ error: "Not found" });

  const messages = await prisma.message.findMany({
    where: { conversationId: id },
    orderBy: [{ createdAt: "desc" }, { id: "desc" }],
    take: limit + 1,
    cursor: cursor ? { id: cursor } : undefined,
    skip: cursor ? 1 : 0,
  });

  const hasMore = messages.length > limit;
  const sliced = hasMore ? messages.slice(0, limit) : messages;
  const reversed = sliced.reverse();

  const prevCursor = reversed[0]?.id || null;
  const nextCursor = hasMore ? messages[messages.length - 1].id : null;

  res.json({
    ...convo,
    messages: reversed,
    pageInfo: { prevCursor, nextCursor },
  });
});

// POST /api/conversations/:id/messages
router.post("/:id/messages", async (req, res) => {
  const { content } = req.body;
  const { id } = req.params;

  console.log("[BACKEND] ========== NEW MESSAGE REQUEST ==========");
  console.log("[BACKEND] Step 1: Request received - POST /messages");
  console.log("[BACKEND] Step 1.1: Conversation ID:", id);
  console.log("[BACKEND] Step 1.2: Content length:", content?.length || 0);
  console.log(
    "[BACKEND] Step 1.3: Content preview:",
    content?.substring(0, 50) || ""
  );
  console.log(
    "[BACKEND] Step 1.4: Request headers:",
    JSON.stringify(req.headers, null, 2)
  );

  // Preferred approach: Client aborts fetch, no server endpoint required
  // When the client aborts the fetch request, the connection closes and we detect it here
  // This cancels the in-flight LLM call automatically
  const abortController = new AbortController();
  let isClientConnected = true;
  let userMsg: any = null;
  let responseSent = false;

  console.log(
    "[BACKEND] Step 2: AbortController created, signal.aborted =",
    abortController.signal.aborted
  );

  // Helper function to check if client is still connected
  const checkConnection = (): boolean => {
    // Check socket state - if destroyed, client disconnected
    // This works even if req.on("aborted") didn't fire (e.g., if body was already read)
    const socket = req.socket;
    const isDestroyed = socket?.destroyed || false;
    const socketExists = !!socket;

    // Socket destroyed = client disconnected
    // No socket = connection never established or already closed
    return socketExists && !isDestroyed;
  };

  // Listen for client disconnect (happens when frontend aborts fetch)
  const cleanup = (eventName: string) => {
    console.log(`[BACKEND] CLEANUP: ${eventName} event fired!`);
    console.log(
      "[BACKEND] CLEANUP: socket.destroyed =",
      req.socket?.destroyed,
      "socket.closed =",
      req.socket?.closed
    );
    console.log(
      "[BACKEND] CLEANUP: isClientConnected =",
      isClientConnected,
      "responseSent =",
      responseSent
    );

    if (isClientConnected && !responseSent) {
      console.log(
        "[BACKEND] CLEANUP: Setting isClientConnected = false and aborting controller"
      );
      isClientConnected = false;
      abortController.abort(); // This will cancel the LLM call
      console.log(
        "[BACKEND] CLEANUP: Controller aborted, signal.aborted =",
        abortController.signal.aborted
      );

      // If user message was created, delete it since request was cancelled
      if (userMsg && !responseSent) {
        console.log(
          "[BACKEND] CLEANUP: Deleting user message from DB, id =",
          userMsg.id
        );
        prisma.message.delete({ where: { id: userMsg.id } }).catch(() => {
          // Ignore deletion errors
        });
      }
    } else {
      console.log(
        "[BACKEND] CLEANUP: Skipping cleanup (already sent response or not connected)"
      );
    }
  };

  // Listen to multiple events to catch disconnects
  // 'aborted' fires when client disconnects before body is fully read
  req.on("aborted", () => cleanup("req.aborted"));

  // 'close' fires when socket closes (but can fire prematurely with keep-alive)
  // We check socket state to filter false positives
  req.on("close", () => {
    // Only treat as disconnect if socket is actually destroyed
    // This filters out premature close events from keep-alive connections
    if (req.socket?.destroyed || req.socket?.closed) {
      cleanup("req.close (socket destroyed)");
    }
  });

  // Also check if socket is already destroyed (immediate disconnect)
  if (!checkConnection()) {
    console.log(
      "[BACKEND] Step 2.5: Socket already destroyed - client disconnected immediately"
    );
    cleanup("immediate check");
    return;
  }

  console.log(
    "[BACKEND] Step 3: Registered req.on('aborted') and req.on('close') listeners"
  );

  // Check if already aborted or disconnected before saving to DB
  const connectionCheck = checkConnection();
  if (
    abortController.signal.aborted ||
    !isClientConnected ||
    !connectionCheck
  ) {
    console.log(
      "[BACKEND] Step 4: Already aborted/disconnected before DB save - exiting",
      {
        signalAborted: abortController.signal.aborted,
        isClientConnected,
        connectionCheck,
        socketDestroyed: req.socket?.destroyed,
      }
    );
    return; // Don't save anything if already cancelled
  }

  console.log("[BACKEND] Step 4: Saving user message to DB");
  userMsg = await prisma.message.create({
    data: { conversationId: id, role: "user", content },
  });
  console.log("[BACKEND] Step 5: User message saved, id =", userMsg.id);

  await prisma.conversation.update({
    where: { id },
    data: { lastMessageAt: new Date() },
  });

  try {
    // Check if cancelled before fetching history
    console.log("[BACKEND] Step 6: Checkpoint #1 - Before fetching history");
    const connectionCheck1 = checkConnection();
    console.log(
      "[BACKEND] Step 6: signal.aborted =",
      abortController.signal.aborted,
      "isClientConnected =",
      isClientConnected,
      "connectionCheck =",
      connectionCheck1
    );
    if (
      abortController.signal.aborted ||
      !isClientConnected ||
      !connectionCheck1
    ) {
      console.log(
        "[BACKEND] Step 6: ABORTED at checkpoint #1 - deleting user message and exiting"
      );
      if (userMsg) {
        await prisma.message.delete({ where: { id: userMsg.id } });
      }
      return;
    }

    // Fetch history
    console.log("[BACKEND] Step 7: Fetching conversation history");
    const dbHistory = await prisma.message.findMany({
      where: { conversationId: id },
      orderBy: { createdAt: "asc" },
      select: { role: true, content: true }, // Only send role + content
    });
    console.log(
      "[BACKEND] Step 8: History fetched, messages count =",
      dbHistory.length
    );

    // Check again after DB fetch
    console.log("[BACKEND] Step 9: Checkpoint #2 - After fetching history");
    const connectionCheck2 = checkConnection();
    console.log(
      "[BACKEND] Step 9: signal.aborted =",
      abortController.signal.aborted,
      "isClientConnected =",
      isClientConnected,
      "connectionCheck =",
      connectionCheck2
    );
    if (
      abortController.signal.aborted ||
      !isClientConnected ||
      !connectionCheck2
    ) {
      console.log(
        "[BACKEND] Step 9: ABORTED at checkpoint #2 - deleting user message and exiting"
      );
      if (userMsg) {
        await prisma.message.delete({ where: { id: userMsg.id } });
      }
      return;
    }

    // Map to LLM format
    const llmHistory = dbHistory.map((m) => ({
      role: m.role as "user" | "assistant", // Type assertion (DB enforces)
      content: m.content,
    }));

    // Pass abort signal to LLM call - if client disconnects, this will cancel the LLM call
    console.log(
      "[BACKEND] Step 10: Calling LLM with signal (signal.aborted =",
      abortController.signal.aborted,
      ")"
    );
    console.log(
      "[BACKEND] Step 10.1: LLM history being sent:",
      JSON.stringify(llmHistory, null, 2)
    );
    console.log("[BACKEND] Step 10.2: LLM adapter type:", llm.constructor.name);

    const llmStartTime = Date.now();
    const completion = await withRetry(
      () => {
        console.log(
          "[BACKEND] Step 10.3: Inside retry function, calling llm.complete()"
        );
        return llm.complete(llmHistory, abortController.signal);
      },
      2,
      500,
      abortController.signal
    );
    const llmDuration = Date.now() - llmStartTime;
    console.log("[BACKEND] Step 11: LLM call completed in", llmDuration, "ms");
    console.log(
      "[BACKEND] Step 11.1: Completion received, length:",
      completion.completion?.length || 0
    );
    console.log(
      "[BACKEND] Step 11.2: Completion preview:",
      completion.completion?.substring(0, 100) || ""
    );

    // Check if cancelled after LLM call
    console.log("[BACKEND] Step 12: Checkpoint #3 - After LLM call");
    const connectionCheck3 = checkConnection();
    console.log(
      "[BACKEND] Step 12: signal.aborted =",
      abortController.signal.aborted,
      "isClientConnected =",
      isClientConnected,
      "connectionCheck =",
      connectionCheck3
    );
    if (
      abortController.signal.aborted ||
      !isClientConnected ||
      !connectionCheck3
    ) {
      console.log(
        "[BACKEND] Step 12: ABORTED at checkpoint #3 - deleting user message and exiting"
      );
      if (userMsg) {
        await prisma.message.delete({ where: { id: userMsg.id } });
      }
      return;
    }

    console.log("[BACKEND] Step 13: Saving assistant message to DB");
    const assistantMsg = await prisma.message.create({
      data: {
        conversationId: id,
        role: "assistant",
        content: completion.completion,
      },
    });
    console.log(
      "[BACKEND] Step 14: Assistant message saved, id =",
      assistantMsg.id
    );

    // Check if client is still connected before sending response
    console.log("[BACKEND] Step 15: Checkpoint #4 - Before sending response");
    const connectionCheck4 = checkConnection();
    console.log(
      "[BACKEND] Step 15: signal.aborted =",
      abortController.signal.aborted,
      "isClientConnected =",
      isClientConnected,
      "connectionCheck =",
      connectionCheck4
    );
    if (
      !isClientConnected ||
      abortController.signal.aborted ||
      !connectionCheck4
    ) {
      console.log(
        "[BACKEND] Step 15: ABORTED at checkpoint #4 - not sending response"
      );
      // Client disconnected/cancelled - don't send response
      return;
    }

    responseSent = true;
    console.log("[BACKEND] Step 16: Sending response to client");
    res.json({
      message: {
        id: userMsg.id,
        role: "user",
        content: userMsg.content,
        createdAt: userMsg.createdAt,
      },
      reply: {
        id: assistantMsg.id,
        role: "assistant",
        content: assistantMsg.content,
        createdAt: assistantMsg.createdAt,
      },
    });
    console.log("[BACKEND] Step 17: Response sent successfully");
  } catch (err: any) {
    console.log("[BACKEND] CATCH: Error caught in try-catch block");
    console.log(
      "[BACKEND] CATCH: Error name =",
      err.name,
      "code =",
      err.code,
      "message =",
      err.message
    );

    // Clean up listeners
    req.removeAllListeners("aborted");
    req.removeAllListeners("close");

    // Ensure we always send a response if headers haven't been sent
    if (res.headersSent) {
      console.log("[BACKEND] CATCH: Headers already sent - returning");
      return;
    }

    responseSent = true;

    // Check if request was aborted/cancelled
    const isAborted =
      abortController.signal.aborted ||
      err.name === "AbortError" ||
      err.name === "CanceledError" ||
      err.code === "ECONNABORTED" ||
      !isClientConnected;

    console.log(
      "[BACKEND] CATCH: isAborted =",
      isAborted,
      "signal.aborted =",
      abortController.signal.aborted
    );

    if (isAborted) {
      console.log(
        "[BACKEND] CATCH: Request was aborted/cancelled - cleaning up"
      );
      // Client cancelled the request - clean up user message from DB
      if (userMsg) {
        console.log("[BACKEND] CATCH: Deleting user message, id =", userMsg.id);
        await prisma.message.delete({ where: { id: userMsg.id } }).catch(() => {
          // Ignore deletion errors
        });
      }
      // Don't send response for cancelled requests
      console.log(
        "[BACKEND] CATCH: Not sending response for cancelled request"
      );
      return;
    }

    // Check for timeout errors
    const isTimeout =
      err.code === "ETIMEDOUT" || err.message?.includes("timeout");

    if (isTimeout) {
      console.log("[BACKEND] CATCH: Timeout error - sending 500 response");
      res.status(500).json({ error: "LLM timeout", retryAfterMs: 1000 });
      return;
    }

    // All other errors (including LLM failures)
    console.error("[BACKEND] CATCH: Other error - sending 500 response", {
      errName: err.name,
      errMessage: err.message,
      errCode: err.code,
      headersSent: res.headersSent,
    });
    res.status(500).json({ error: "LLM failed", retryAfterMs: 1000 });
  }
});

export default router;


--------------------------------------------------------------------------------

--- FILE: backend\src\utils\retry.ts ---
# Path: C:\vs code projects\mini ChatGpt\backend\src\utils\retry.ts
# Size: 2681 bytes

export async function withRetry<T>(
  fn: () => Promise<T>,
  retries: number,
  delayMs: number,
  signal?: AbortSignal
): Promise<T> {
  console.log('[RETRY] Starting retry wrapper, max retries:', retries, 'delayMs:', delayMs);
  let lastError;
  
  for (let i = 0; i <= retries; i++) {
    console.log('[RETRY] Attempt', i + 1, 'of', retries + 1);
    
    // Check if aborted before retry
    if (signal?.aborted) {
      console.log('[RETRY] Signal already aborted before attempt', i + 1);
      const abortError = new Error("Aborted");
      abortError.name = "AbortError";
      throw abortError;
    }

    try {
      console.log('[RETRY] Calling function...');
      const startTime = Date.now();
      const result = await fn();
      const duration = Date.now() - startTime;
      console.log('[RETRY] ✅ Function succeeded on attempt', i + 1, 'after', duration, 'ms');
      return result;
    } catch (err: any) {
      lastError = err;
      console.error('[RETRY] ❌ Attempt', i + 1, 'failed');
      console.error('[RETRY] Error name:', err.name);
      console.error('[RETRY] Error message:', err.message);
      console.error('[RETRY] Error code:', err.code);

      // Don't retry if aborted (axios throws CanceledError or AbortError)
      if (
        err.name === "AbortError" ||
        err.name === "CanceledError" ||
        signal?.aborted
      ) {
        console.log('[RETRY] Request was aborted - not retrying');
        const abortError = new Error("Aborted");
        abortError.name = "AbortError";
        throw abortError;
      }

      if (i < retries) {
        const delay = delayMs * (i + 1);
        console.log('[RETRY] Waiting', delay, 'ms before retry...');
        // Check abort during delay
        await new Promise((r, reject) => {
          const timeout = setTimeout(() => {
            signal?.removeEventListener("abort", abortHandler);
            console.log('[RETRY] Delay complete, proceeding to retry');
            r(undefined);
          }, delay);

          const abortHandler = () => {
            console.log('[RETRY] Aborted during delay');
            clearTimeout(timeout);
            signal?.removeEventListener("abort", abortHandler);
            const abortError = new Error("Aborted");
            abortError.name = "AbortError";
            reject(abortError);
          };

          signal?.addEventListener("abort", abortHandler);
        });
      } else {
        console.error('[RETRY] All retries exhausted');
      }
    }
  }
  console.error('[RETRY] Throwing last error after all retries failed');
  throw lastError;
}


--------------------------------------------------------------------------------


# TOTAL FILES COPIED: 40
# Finished at: 2025-11-01 13:10:11
