services:
  # === DATABASE: PostgreSQL (Persistent) ===
  db:
    image: postgres:16
    container_name: chatgpt-db
    environment:
      POSTGRES_DB: chatgpt
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s

  # === MOCK LLM (Provided) ===
  mock-llm:
    build: ./mock-llm
    container_name: mock-llm
    ports:
      - "8080:8080"
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:8080/health",
        ]
      interval: 2s
      timeout: 2s
      retries: 5
      start_period: 5s

  # === OLLAMA: Local LLM Service ===
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      # Increase timeout for model processing
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_LOAD_TIMEOUT=10m
      # Allow more time for model loading
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      # Debug logging to see what's happening
      - OLLAMA_DEBUG=true
    # Add resource limits if needed (optional - comment out if you want unlimited)
    # deploy:
    #   resources:
    #     limits:
    #       memory: 8G
    #     reservations:
    #       memory: 4G

  # === OLLAMA INIT: Pull model automatically (only if missing) ===
  ollama-init:
    image: curlimages/curl:latest
    container_name: ollama-init
    depends_on:
      ollama:
        condition: service_started
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for Ollama to be ready..."
        until curl -f http://ollama:11434/api/tags 2>/dev/null; do
          echo "Waiting for Ollama..."
          sleep 2
        done
        echo "Ollama is ready. Checking if model tinyllama exists..."
        MODELS=$$(curl -s http://ollama:11434/api/tags)
        if echo "$$MODELS" | grep -q '"name":"tinyllama"'; then
          echo "Model tinyllama already exists - no download needed."
          exit 0
        fi
        echo "Model tinyllama not found. Pulling (this may take several minutes)..."
        echo "NOTE: This only happens once - the model is saved and reused on future runs."
        echo "tinyllama (~637MB) is one of the FASTEST models - optimized for speed!"
        curl -X POST http://ollama:11434/api/pull -d '{"name":"tinyllama"}' --no-buffer
        echo "Model tinyllama pulled successfully!"

  # === BACKEND: Node.js + Express + Prisma ===
  backend:
    build: ./backend
    container_name: chatgpt-backend
    ports:
      - "3001:3001"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/chatgpt?schema=public

      # === MOCK LLM CONFIGURATION ===
      # To use Mock LLM: uncomment the 2 lines below
      # - LLM_PROVIDER=mock
      # - MOCK_LLM_BASE_URL=http://mock-llm:8080

      # === OLLAMA CONFIGURATION ===
      # To use Ollama: uncomment the 3 lines below
      # - LLM_PROVIDER=ollama
      # - OLLAMA_BASE_URL=http://ollama:11434
      # - OLLAMA_MODEL=tinyllama

    depends_on:
      db:
        condition: service_healthy
      # === MOCK LLM DEPENDENCY ===
      # Uncomment when using Mock LLM (comment out ollama-init below)
      mock-llm:
        condition: service_healthy

      # === OLLAMA DEPENDENCY ===
      # Uncomment when using Ollama (comment out mock-llm above)
      # ollama-init:
      #   condition: service_completed_successfully
    command: >
      sh -c "
        echo 'Waiting for DB...' &&
        until nc -z db 5432; do sleep 1; done &&
        echo 'DB ready. Checking Mock LLM...' &&
        COUNTER=0 &&
        while ! nc -z mock-llm 8080 2>/dev/null && [ $$COUNTER -lt 10 ]; do
          echo 'Waiting for Mock LLM...' &&
          sleep 1 &&
          COUNTER=$$((COUNTER + 1))
        done &&
        if nc -z mock-llm 8080 2>/dev/null; then
          echo 'Mock LLM ready.'
        else
          echo 'Mock LLM not available (will proceed anyway).'
        fi &&
        echo 'Running migrations...' &&
        npx prisma migrate deploy &&
        echo 'Starting backend...' &&
        npm start
      "

  # === FRONTEND: React + MUI + Nginx ===
  frontend:
    build: ./front
    container_name: chatgpt-frontend
    ports:
      - "3000:80"
    depends_on:
      - backend

volumes:
  postgres_data:
  ollama_data:
